{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "from asyncio import MultiLoopChildWatcher\n",
    "from doctest import OutputChecker\n",
    "\n",
    "from turtle import hideturtle\n",
    "import warnings\n",
    "\n",
    "from models import GeneralModel\n",
    "from models.statistics.Metrics import Metrics\n",
    "from utils.config_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.system_utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from models.networks.ConvertMLP import ConvertMLP2,ConvertMLP3\n",
    "\n",
    "from verify_utils.verify_utils import verify_single_image\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cu113'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define arguments manually\n",
    "arguments = argparse.Namespace()\n",
    "# device\n",
    "arguments.device = \"cuda\"\n",
    "\n",
    "# define arguments for model\n",
    "#arguments.model = \"ResNet18\" # ResNet not supported for structured\n",
    "arguments.model = \"MLP2\"\n",
    "arguments.hidden_dim = 64\n",
    "#arguments.input_dim = None # for ResNet\n",
    "#arguments.input_dim = (1,1,1) # for LeNet5\n",
    "arguments.input_dim = (28,28) # for MNIST\n",
    "arguments.output_dim = 10\n",
    "arguments.disable_masking = 1 # 0 for disable mask, 1 for mask (unstructured)\n",
    "arguments.track_weights = 0\n",
    "arguments.enable_rewinding = 0\n",
    "arguments.growing_rate = 0.0000\n",
    "arguments.outer_layer_pruning = 0\n",
    "# arguments.prune_criterion = \"SNIPit\"  # unstructured\n",
    "\n",
    "arguments.prune_criterion = \"SNAPit\" # or SNAPit ... # structured\n",
    "arguments.l0 = 0\n",
    "arguments.l0_reg = 1.0\n",
    "arguments.l1_reg = 0\n",
    "arguments.lp_reg = 0\n",
    "arguments.l2_reg = 5e-5\n",
    "arguments.hoyer_reg = 0.001\n",
    "arguments.N = 6000 # different for different dataset\n",
    "arguments.beta_ema = 0.999\n",
    "\n",
    "\n",
    "# define arguments for criterion\n",
    "arguments.pruning_limit = 0.5\n",
    "arguments.snip_steps = 6\n",
    "\n",
    "# not pre-trained model\n",
    "arguments.checkpoint_name = None\n",
    "arguments.checkpoint_model = None\n",
    "\n",
    "# dataset\n",
    "arguments.data_set = \"MNIST\"\n",
    "arguments.batch_size = 512\n",
    "arguments.mean = (0.1307,)\n",
    "arguments.std = (0.3081,)\n",
    "arguments.tuning = 0\n",
    "arguments.preload_all_data = 0\n",
    "arguments.random_shuffle_labels = 0\n",
    "\n",
    "# loss\n",
    "arguments.loss = \"CrossEntropy\"\n",
    "\n",
    "# optimizer\n",
    "arguments.optimizer = \"ADAM\"\n",
    "arguments.learning_rate = 2e-3\n",
    "\n",
    "# training\n",
    "arguments.save_freq = 1e6\n",
    "arguments.eval = 0\n",
    "arguments.train_scheme = \"DefaultTrainer\"\n",
    "arguments.seed = 1234\n",
    "arguments.epochs = 5\n",
    "\n",
    "arguments.grad_noise = 0\n",
    "arguments.grad_clip =10\n",
    "arguments.eval_freq = 1000\n",
    "arguments.max_training_minutes= 6120\n",
    "arguments.plot_weights_freq = 50\n",
    "arguments.prune_delay = 0\n",
    "arguments.prune_freq = 1\n",
    "arguments.rewind_to = 6\n",
    "\n",
    "arguments.skip_first_plot = 0\n",
    "arguments.disable_histograms = 0\n",
    "arguments.disable_saliency = 0\n",
    "arguments.disable_confusion = 0\n",
    "arguments.disable_weightplot = 0\n",
    "arguments.disable_netplot = 0\n",
    "arguments.disable_activations = 0\n",
    "\n",
    "arguments.pruning_rate = 0\n",
    "# during training\n",
    "arguments.pruning_freq = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at 2022-04-28_17.51.09\n"
     ]
    }
   ],
   "source": [
    "metrics = Metrics()\n",
    "out = metrics.log_line\n",
    "print = out\n",
    "\n",
    "ensure_current_directory()\n",
    "global out \n",
    "out = metrics.log_line\n",
    "out(f\"starting at {get_date_stamp()}\")\n",
    "\n",
    "metrics._batch_size = arguments.batch_size\n",
    "metrics._eval_freq = arguments.eval_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = configure_device(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model\n",
    "model: GeneralModel = find_right_model(\n",
    "        NETWORKS_DIR,arguments.model,\n",
    "        device=device,\n",
    "        hidden_dim = arguments.hidden_dim,\n",
    "        input_dim = arguments.input_dim,\n",
    "        output_dim = arguments.output_dim,\n",
    "        is_maskable=arguments.disable_masking,\n",
    "        is_tracking_weights=arguments.track_weights,\n",
    "        is_rewindable=arguments.enable_rewinding,\n",
    "        is_growable=arguments.growing_rate > 0,\n",
    "        outer_layer_pruning=arguments.outer_layer_pruning,\n",
    "        maintain_outer_mask_anyway=(\n",
    "                                       not arguments.outer_layer_pruning) and (\n",
    "                                           \"Structured\" in arguments.prune_criterion),\n",
    "        l0=arguments.l0,\n",
    "        l0_reg=arguments.l0_reg,\n",
    "        N=arguments.N,\n",
    "        beta_ema=arguments.beta_ema,\n",
    "        l2_reg=arguments.l2_reg\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n",
      "torch.Size([64])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for key,param in model.named_parameters():\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get criterion\n",
    "criterion = find_right_model(\n",
    "        CRITERION_DIR,arguments.prune_criterion,\n",
    "        model=model,\n",
    "        limit=arguments.pruning_limit,\n",
    "        start=0.5,\n",
    "        steps=arguments.snip_steps,\n",
    "        device=arguments.device\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(arguments, metrics, model):\n",
    "    if (not (arguments.checkpoint_name is None)) and (not (arguments.checkpoint_model is None)):\n",
    "        path = os.path.join(RESULTS_DIR, arguments.checkpoint_name, MODELS_DIR, arguments.checkpoint_model)\n",
    "        state = DATA_MANAGER.load_python_obj(path)\n",
    "        try:\n",
    "            model.load_state_dict(state)\n",
    "        except KeyError as e:\n",
    "            print(list(state.keys()))\n",
    "            raise e\n",
    "        out(f\"Loaded checkpoint {arguments.checkpoint_name} from {arguments.checkpoint_model}\")\n",
    "\n",
    "# load pre-trained weights if specified\n",
    "load_checkpoint(arguments, metrics, model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mean (0.1307,)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_loader, test_loader = find_right_model(\n",
    "        DATASETS, arguments.data_set,\n",
    "        arguments=arguments,\n",
    "        mean=arguments.mean,\n",
    "        std=arguments.std\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get loss function\n",
    "loss = find_right_model(\n",
    "        LOSS_DIR, arguments.loss,\n",
    "        device=device,\n",
    "        l1_reg=arguments.l1_reg,\n",
    "        lp_reg=arguments.lp_reg,\n",
    "        l0_reg=arguments.l0_reg,\n",
    "        hoyer_reg=arguments.hoyer_reg\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optimizer\n",
    "optimizer = find_right_model(\n",
    "        OPTIMS, arguments.optimizer,\n",
    "        params=model.parameters(),\n",
    "        lr=arguments.learning_rate,\n",
    "        weight_decay=arguments.l2_reg if not arguments.l0 else 0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made datestamp: 2022-04-28_17.51.14_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.5_train-scheme=DefaultTrainer_seed=1234\n"
     ]
    }
   ],
   "source": [
    "if not arguments.eval:\n",
    "    # build trainer\n",
    "    run_name = f'_model={arguments.model}_dataset={arguments.data_set}_prune-criterion={arguments.prune_criterion}' + \\\n",
    "               f'_pruning-limit={arguments.pruning_limit}_train-scheme={arguments.train_scheme}_seed={arguments.seed}'\n",
    "    trainer = find_right_model(\n",
    "            TRAINERS_DIR, arguments.train_scheme,\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            arguments=arguments,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            metrics=metrics,\n",
    "            criterion=criterion,\n",
    "            run_name = run_name\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mStarted training\u001b[0m\n",
      "Saved results/2022-04-28_17.51.14_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.5_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 25088 percentage 0.5 length_nonzero 50176\n",
      "pruning 320 percentage 0.5 length_nonzero 640\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=32.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=32.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.5\n",
      "Saved results/2022-04-28_17.51.14_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.5_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 784 percentage 0.03125 length_nonzero 25088\n",
      "pruning 10 percentage 0.03125 length_nonzero 320\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=31.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=31.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.03125\n",
      "Saved results/2022-04-28_17.51.14_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.5_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 0 percentage 0.0 length_nonzero 24304\n",
      "pruning 0 percentage 0.0 length_nonzero 310\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=31.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=31.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.0\n",
      "Saved results/2022-04-28_17.51.14_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.5_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 0 percentage 0.0 length_nonzero 24304\n",
      "pruning 0 percentage 0.0 length_nonzero 310\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=31.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=31.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.0\n",
      "Saved results/2022-04-28_17.51.14_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.5_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 0 percentage 0.0 length_nonzero 24304\n",
      "pruning 0 percentage 0.0 length_nonzero 310\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=31.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=31.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.0\n",
      "Saved results/2022-04-28_17.51.14_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.5_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 0 percentage 0.0 length_nonzero 24304\n",
      "pruning 0 percentage 0.0 length_nonzero 310\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=31.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=31.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.0\n",
      "Saved results/2022-04-28_17.51.14_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.5_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 0 percentage 0.0 length_nonzero 24304\n",
      "pruning 0 percentage 0.0 length_nonzero 310\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=31.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=31.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.0\n",
      "Saved results/2022-04-28_17.51.14_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.5_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 0 percentage 0.0 length_nonzero 24304\n",
      "pruning 0 percentage 0.0 length_nonzero 310\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=31.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=31.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.0\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 0 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.1289062  |  2.4451895   |  2.1467733  | 0.2300954  |    0.5156250    |   0.5156250   |  0.3181968  |       13.6375339       |    0.7920671    |      24903.0000000      |      7.1055216      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    450560.0000000    |     0.0230568     |\n",
      "Training... 117/118\n",
      "\n",
      "plotting..\n",
      "finished plotting\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 1 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9238281  |  0.2736609   |  0.2758058  | 0.9207950  |    0.5156250    |   0.5156250   |  0.6610670  |       13.6375339       |    2.4866224    |      24903.0000000      |      9.1810686      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    450560.0000000    |     0.0085309     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 2 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9433594  |  0.1926165   |  0.2228933  | 0.9358054  |    0.5156250    |   0.5156250   |  0.6648953  |       13.6375339       |    2.4649176    |      24903.0000000      |      9.4802700      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    450560.0000000    |     0.0085809     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 3 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9433594  |  0.1885930   |  0.1858527  | 0.9465993  |    0.5156250    |   0.5156250   |  0.6675997  |       13.6375339       |    2.1945118    |      24903.0000000      |      9.6557500      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    450560.0000000    |     0.0085450     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 4 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9609375  |  0.1329588   |  0.1648312  | 0.9504940  |    0.5156250    |   0.5156250   |  0.6685657  |       13.6375339       |    2.4501783    |      24903.0000000      |      9.7803828      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    450560.0000000    |     0.0085519     |\n",
      "Training... 117/118\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31, 784])\n",
      "torch.Size([31])\n",
      "torch.Size([10, 31])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for key,param in model.named_parameters():\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ConvertMLP2.fuse of ConvertMLP2(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=31, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=31, out_features=10, bias=True)\n",
       "  )\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change to cpu and eval mode\n",
    "model.to(\"cpu\").eval()\n",
    "\n",
    "# get the model parameters\n",
    "state = model.state_dict()\n",
    "# convet the model to a quantizable model\n",
    "q_model= ConvertMLP2(model)\n",
    "# load previous parameter\n",
    "q_model.load_state_dict(state)\n",
    "# set quantization config\n",
    "q_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "# fuse_model\n",
    "q_model.fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.3108,  -9.7951,   0.0323,  ...,  -3.1140,  -0.2278,   2.6842],\n",
       "        [ -4.2946,  -1.7511,   3.1539,  ..., -12.1547,   2.3074,  -6.1053],\n",
       "        [ -5.1860,  -4.1389,   0.5035,  ...,  -2.7515,   7.0176,  -1.4935],\n",
       "        ...,\n",
       "        [ -6.9084,  -9.5606,  -0.4408,  ...,  -8.0227,   5.7194,   0.6637],\n",
       "        [  9.1948,  -9.9583,   3.2057,  ...,  -0.8465,  -0.0597,  -1.1003],\n",
       "        [ -1.7863, -11.0944,   1.8253,  ...,   1.1026,  -0.5046,   0.8225]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use calibrate date to calibrate model\n",
    "model_prepared = torch.quantization.prepare(q_model)\n",
    "cali_data,label=next(iter(train_loader))\n",
    "model_prepared(cali_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to int8\n",
    "model_int8 = torch.quantization.convert(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvertMLP2(\n",
       "  (layers): Sequential(\n",
       "    (0): QuantizedLinear(in_features=784, out_features=31, scale=0.38516145944595337, zero_point=69, qscheme=torch.per_channel_affine)\n",
       "    (1): ReLU()\n",
       "    (2): QuantizedLinear(in_features=31, out_features=10, scale=0.3201819658279419, zero_point=68, qscheme=torch.per_channel_affine)\n",
       "  )\n",
       "  (quant): Quantize(scale=tensor([0.0255]), zero_point=tensor([16]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the model size\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32model  \t Size (KB): 100.127\n",
      "model:  quantized  \t Size (KB): 29.691\n"
     ]
    }
   ],
   "source": [
    "size_model_fp32 = print_size_of_model(model,label=\"fp32model\")\n",
    "size_model_int8 = print_size_of_model(model_int8,label=\"quantized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to verify a quantized model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a test image \n",
    "images,label=next(iter(test_loader))\n",
    "img = images[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first convert the input to int\n",
    "int_input=model_int8.quant(img).int_repr().view(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight_int = model_int8.layers[0]._packed_params._weight_bias()[0].int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_int=model_int8.layers[0].weight().int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38516145944595337 69\n"
     ]
    }
   ],
   "source": [
    "weight,bias=model_int8.layers[0]._packed_params._weight_bias()\n",
    "scale=model_int8.layers[0].scale\n",
    "zero_point = model_int8.layers[0].zero_point\n",
    "print(scale,zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, Tracer, AutocastCPU, Autocast, Batched, VmapMode, Functionalize].\n\nQuantizedCPU: registered at ../aten/src/ATen/native/quantized/cpu/qlinear.cpp:455 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:35 [backend fallback]\nAutogradCPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:39 [backend fallback]\nAutogradCUDA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:47 [backend fallback]\nAutogradXLA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:51 [backend fallback]\nAutogradLazy: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:55 [backend fallback]\nAutogradXPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:43 [backend fallback]\nAutogradMLC: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:59 [backend fallback]\nAutogradHPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:68 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:293 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:461 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1059 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:52 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-ddb090d516b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m_packed_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_prepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_packed_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, Tracer, AutocastCPU, Autocast, Batched, VmapMode, Functionalize].\n\nQuantizedCPU: registered at ../aten/src/ATen/native/quantized/cpu/qlinear.cpp:455 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:35 [backend fallback]\nAutogradCPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:39 [backend fallback]\nAutogradCUDA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:47 [backend fallback]\nAutogradXLA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:51 [backend fallback]\nAutogradLazy: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:55 [backend fallback]\nAutogradXPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:43 [backend fallback]\nAutogradMLC: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:59 [backend fallback]\nAutogradHPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:68 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:293 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:461 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1059 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:52 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "_packed_params = torch.ops.quantized.linear_prepack(weight, bias)\n",
    "torch.ops.quantized.linear(int_input, _packed_params, scale, zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'layers',\n",
       " 'layers.0',\n",
       " 'layers.0._packed_params',\n",
       " 'layers.1',\n",
       " 'layers.2',\n",
       " 'layers.2._packed_params',\n",
       " 'quant',\n",
       " 'dequant']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params=[]\n",
    "names=[]\n",
    "for name,module in model_int8.named_modules():\n",
    "    names.append(name)\n",
    "    if \"params\" in name:\n",
    "        params.append(module)\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1145, -0.0442, -0.0409,  ..., -0.0785, -0.0376, -0.0703],\n",
       "        [-0.0074,  0.0321,  0.0296,  ..., -0.0395, -0.0444,  0.0469],\n",
       "        [ 0.0062, -0.0348,  0.0232,  ..., -0.0366,  0.0473,  0.0205],\n",
       "        ...,\n",
       "        [-0.0715, -0.0447,  0.0179,  ...,  0.0067, -0.0335, -0.0737],\n",
       "        [-0.0607,  0.0000, -0.0214,  ..., -0.0464, -0.0161, -0.0607],\n",
       "        [ 0.0200,  0.0360, -0.0380,  ..., -0.0400,  0.0140,  0.0200]],\n",
       "       size=(31, 784), dtype=torch.qint8,\n",
       "       quantization_scheme=torch.per_channel_affine,\n",
       "       scale=tensor([0.0016, 0.0025, 0.0009, 0.0027, 0.0016, 0.0018, 0.0016, 0.0016, 0.0019,\n",
       "        0.0025, 0.0016, 0.0015, 0.0009, 0.0023, 0.0011, 0.0020, 0.0022, 0.0020,\n",
       "        0.0019, 0.0011, 0.0017, 0.0018, 0.0028, 0.0017, 0.0017, 0.0017, 0.0019,\n",
       "        0.0020, 0.0022, 0.0018, 0.0020], dtype=torch.float64),\n",
       "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0]),\n",
       "       axis=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_tuple=params[0]._weight_bias()\n",
    "weight_tuple[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the first layer\n",
    "# get the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test single image verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found solution\n"
     ]
    }
   ],
   "source": [
    "img,label=next(iter(test_loader))\n",
    "img_test = img[3].squeeze().to(device)\n",
    "model.to(device)\n",
    "img,label,indicator=verify_single_image(model=model,image=img_test,eps=0.2)\n",
    "if indicator==0:\n",
    "    print(\"verified!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.value.reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"perturbed prediction: {label.value.argmax()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi-processing to get robustness accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verify_utils.verify_utils import verify_batch_images\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare time without multi-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "accuracy:0.966796875\n",
      "time elapsed 962.2611734867096s\n"
     ]
    }
   ],
   "source": [
    "# a seririalized implementation\n",
    "start = time.time()\n",
    "img_number=0\n",
    "verified_number=0\n",
    "imgs,labels=next(iter(test_loader))\n",
    "eps = 0.01\n",
    "model.to(device)\n",
    "img_number += len(imgs)\n",
    "for img in imgs:\n",
    "    img = img.squeeze().to(device)\n",
    "    _,_,indicator = verify_single_image(model=model,image=img,eps=eps)\n",
    "    if indicator==0:\n",
    "        verified_number+=1\n",
    "finish=time.time()\n",
    "print(f\"accuracy:{verified_number/img_number}\")\n",
    "print(f\"time elapsed {finish-start}s\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_num:512,verified_num:495\n"
     ]
    }
   ],
   "source": [
    "print(f\"total_num:{img_number},verified_num:{verified_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Value,Lock\n",
    "import time\n",
    "from verify_utils.verify_utils import verify_batch_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "total_num:512,verified_num:495\n",
      "accuracy:0.966796875\n",
      "time elapsed 547.378613948822s\n"
     ]
    }
   ],
   "source": [
    "# multi processing\n",
    "start = time.time()\n",
    "# step1 get a batch images\n",
    "imgs,labels=next(iter(test_loader))\n",
    "model.to(device)\n",
    "# start from a GPU model and tensor imgs\n",
    "# step2 get the labels for a batch imgs\n",
    "labels=model(imgs.to(device)).argmax(axis=1).cpu().numpy()\n",
    "# step 3 split imgs and labels for each process\n",
    "num_process =10\n",
    "processes=[]\n",
    "num_per_process=int(len(imgs)/num_process)\n",
    "# convert model to cpu\n",
    "model.to(\"cpu\")\n",
    "# define processes\n",
    "lock =Lock()\n",
    "total_num = Value('i',0)\n",
    "verified_num = Value('i',0)\n",
    "eps=0.01\n",
    "for i in range(num_process):\n",
    "    if i!=num_process-1:\n",
    "        process_imgs=imgs[num_per_process*i:num_per_process*(i+1)].squeeze()\n",
    "        process_labels = labels[num_per_process*i:num_per_process*(i+1)]\n",
    "    else:\n",
    "        process_imgs=imgs[num_per_process*i:].squeeze()\n",
    "        process_labels = labels[num_per_process*i:]\n",
    "    p=Process(target=verify_batch_images,args=(model,process_imgs,process_labels,eps,total_num,verified_num,lock))\n",
    "    processes.append(p)\n",
    "    \n",
    "# start\n",
    "for p in processes:\n",
    "    p.start()\n",
    "    \n",
    "# join\n",
    "for p in processes:\n",
    "    p.join()\n",
    "finish = time.time()\n",
    "\n",
    "print(f\"total_num:{total_num.value},verified_num:{verified_num.value}\")\n",
    "print(f\"accuracy:{verified_num.value/total_num.value}\")\n",
    "print(f\"time elapsed {finish-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2  cpu 526.1203780174255s\n",
    "# 4  cpu 534.806479215622s\n",
    "# 10 cpu 548.1252098083496s 547.378613948822s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
