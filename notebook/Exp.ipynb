{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "from asyncio import MultiLoopChildWatcher\n",
    "from doctest import OutputChecker\n",
    "\n",
    "from turtle import hideturtle\n",
    "import warnings\n",
    "\n",
    "from models import GeneralModel\n",
    "from models.statistics.Metrics import Metrics\n",
    "from utils.config_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.system_utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from models.networks.ConvertMLP import ConvertMLP2,ConvertMLP3\n",
    "\n",
    "from verify_utils.verify_utils import verify_single_image\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cu113'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define arguments manually\n",
    "arguments = {}\n",
    "# device\n",
    "arguments['device'] = \"cuda\"\n",
    "\n",
    "# define arguments for model\n",
    "#arguments.model = \"ResNet18\" # ResNet not supported for structured\n",
    "arguments['model'] = \"MLP2\"\n",
    "arguments['hidden_dim'] = 64\n",
    "#arguments.input_dim = None # for ResNet\n",
    "#arguments.input_dim = (1,1,1) # for LeNet5\n",
    "arguments['input_dim'] = (28,28) # for MNIST\n",
    "arguments['output_dim'] = 10\n",
    "arguments['disable_masking'] = 1 # 0 for disable mask, 1 for mask (unstructured)\n",
    "arguments['track_weights'] = 0\n",
    "arguments['enable_rewinding'] = 0\n",
    "arguments['growing_rate'] = 0.0000\n",
    "arguments['outer_layer_pruning'] = 0\n",
    "# arguments.prune_criterion = \"SNIPit\"  # unstructured\n",
    "\n",
    "arguments['prune_criterion'] = \"SNAPit\" # or SNAPit ... # structured\n",
    "arguments['l0'] = 0\n",
    "arguments['l0_reg'] = 1.0\n",
    "arguments['l1_reg'] = 0\n",
    "arguments['lp_reg'] = 0\n",
    "arguments['l2_reg'] = 5e-5\n",
    "arguments['hoyer_reg'] = 0.001\n",
    "arguments['N'] = 6000 # different for different dataset\n",
    "arguments['beta_ema'] = 0.999\n",
    "\n",
    "\n",
    "# define arguments for criterion\n",
    "arguments['pruning_limit'] = 0.75\n",
    "arguments['snip_steps'] = 6\n",
    "\n",
    "# not pre-trained model\n",
    "arguments['checkpoint_name'] = None\n",
    "arguments['checkpoint_model'] = None\n",
    "\n",
    "# dataset\n",
    "arguments['data_set'] = \"MNIST\"\n",
    "arguments['batch_size'] = 512\n",
    "arguments['mean'] = (0.1307,)\n",
    "arguments['std'] = (0.3081,)\n",
    "arguments['tuning'] = 0\n",
    "arguments['preload_all_data'] = 0\n",
    "arguments['random_shuffle_labels'] = 0\n",
    "\n",
    "# loss\n",
    "arguments['loss'] = \"CrossEntropy\"\n",
    "\n",
    "# optimizer\n",
    "arguments['optimizer'] = \"ADAM\"\n",
    "arguments['learning_rate'] = 2e-3\n",
    "\n",
    "# training\n",
    "arguments['save_freq'] = 2\n",
    "arguments['eval'] = 0\n",
    "arguments['train_scheme'] = \"DefaultTrainer\"\n",
    "arguments['seed'] = 1234\n",
    "arguments['epochs'] = 6\n",
    "\n",
    "arguments['grad_noise'] = 0\n",
    "arguments['grad_clip'] =10\n",
    "arguments['eval_freq'] = 1000\n",
    "arguments['max_training_minutes']= 6120\n",
    "arguments['plot_weights_freq'] = 50\n",
    "arguments['prune_delay'] = 0\n",
    "arguments['prune_freq'] = 1\n",
    "arguments['rewind_to'] = 6\n",
    "\n",
    "arguments['skip_first_plot'] = 0\n",
    "arguments['disable_histograms'] = 0\n",
    "arguments['disable_saliency'] = 0\n",
    "arguments['disable_confusion'] = 0\n",
    "arguments['disable_weightplot'] = 0\n",
    "arguments['disable_netplot'] = 0\n",
    "arguments['disable_activations'] = 0\n",
    "\n",
    "arguments['pruning_rate'] = 0\n",
    "# during training\n",
    "arguments['pruning_freq'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at 2022-05-18_17.38.00\n"
     ]
    }
   ],
   "source": [
    "metrics = Metrics()\n",
    "out = metrics.log_line\n",
    "print = out\n",
    "\n",
    "ensure_current_directory()\n",
    "global out \n",
    "out = metrics.log_line\n",
    "out(f\"starting at {get_date_stamp()}\")\n",
    "\n",
    "metrics._batch_size = arguments['batch_size']\n",
    "metrics._eval_freq = arguments['eval_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = configure_device(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model\n",
    "model: GeneralModel = find_right_model(\n",
    "        NETWORKS_DIR,arguments['model'],\n",
    "        device=device,\n",
    "        hidden_dim = arguments['hidden_dim'],\n",
    "        input_dim = arguments['input_dim'],\n",
    "        output_dim = arguments['output_dim'],\n",
    "        is_maskable=arguments['disable_masking'],\n",
    "        is_tracking_weights=arguments['track_weights'],\n",
    "        is_rewindable=arguments['enable_rewinding'],\n",
    "        is_growable=arguments['growing_rate'] > 0,\n",
    "        outer_layer_pruning=arguments['outer_layer_pruning'],\n",
    "        maintain_outer_mask_anyway=(\n",
    "                                       not arguments['outer_layer_pruning']) and (\n",
    "                                           \"Structured\" in arguments['prune_criterion']),\n",
    "        l0=arguments['l0'],\n",
    "        l0_reg=arguments['l0_reg'],\n",
    "        N=arguments['N'],\n",
    "        beta_ema=arguments['beta_ema'],\n",
    "        l2_reg=arguments['l2_reg']\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n",
      "torch.Size([64])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for key,param in model.named_parameters():\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get criterion\n",
    "criterion = find_right_model(\n",
    "        CRITERION_DIR,arguments['prune_criterion'],\n",
    "        model=model,\n",
    "        limit=arguments['pruning_limit'],\n",
    "        start=0.5,\n",
    "        steps=arguments['snip_steps'],\n",
    "        device=arguments['device']\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mean (0.1307,)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_loader, test_loader = find_right_model(\n",
    "        DATASETS, arguments['data_set'],\n",
    "        arguments=arguments,\n",
    "        mean=arguments['mean'],\n",
    "        std=arguments['std']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get loss function\n",
    "loss = find_right_model(\n",
    "        LOSS_DIR, arguments['loss'],\n",
    "        device=device,\n",
    "        l1_reg=arguments['l1_reg'],\n",
    "        lp_reg=arguments['lp_reg'],\n",
    "        l0_reg=arguments['l0_reg'],\n",
    "        hoyer_reg=arguments['hoyer_reg']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optimizer\n",
    "optimizer = find_right_model(\n",
    "        OPTIMS, arguments['optimizer'],\n",
    "        params=model.parameters(),\n",
    "        lr=arguments['learning_rate'],\n",
    "        weight_decay=arguments['l2_reg'] if not arguments['l0'] else 0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made datestamp: 2022-05-18_17.38.04_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.75_train-scheme=DefaultTrainer_seed=1234\n"
     ]
    }
   ],
   "source": [
    "if not arguments['eval']:\n",
    "    # build trainer\n",
    "    run_name = f\"_model={arguments['model']}_dataset={arguments['data_set']}_prune-criterion={arguments['prune_criterion']}\" + \\\n",
    "               f\"_pruning-limit={arguments['pruning_limit']}_train-scheme={arguments['train_scheme']}_seed={arguments['seed']}\"\n",
    "    trainer = find_right_model(\n",
    "            TRAINERS_DIR, arguments['train_scheme'],\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            arguments=arguments,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            metrics=metrics,\n",
    "            criterion=criterion,\n",
    "            run_name = run_name\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mStarted training\u001b[0m\n",
      "Saved results/2022-05-18_17.38.04_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.75_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 25088 percentage 0.5 length_nonzero 50176\n",
      "pruning 320 percentage 0.5 length_nonzero 640\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=32.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=32.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.5\n",
      "Saved results/2022-05-18_17.38.04_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.75_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 7056 percentage 0.28125 length_nonzero 25088\n",
      "pruning 90 percentage 0.28125 length_nonzero 320\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=23.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=23.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.28125\n",
      "Saved results/2022-05-18_17.38.04_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.75_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 3136 percentage 0.17391304347826086 length_nonzero 18032\n",
      "pruning 40 percentage 0.17391304347826086 length_nonzero 230\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=19.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=19.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.17391304347826086\n",
      "Saved results/2022-05-18_17.38.04_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.75_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 1568 percentage 0.10526315789473684 length_nonzero 14896\n",
      "pruning 20 percentage 0.10526315789473684 length_nonzero 190\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=17.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=17.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.10526315789473684\n",
      "Saved results/2022-05-18_17.38.04_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.75_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 784 percentage 0.058823529411764705 length_nonzero 13328\n",
      "pruning 10 percentage 0.058823529411764705 length_nonzero 170\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=16.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=16.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.058823529411764705\n",
      "Saved results/2022-05-18_17.38.04_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.75_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 0 percentage 0.0 length_nonzero 12544\n",
      "pruning 0 percentage 0.0 length_nonzero 160\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=16.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=16.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.0\n",
      "Saved results/2022-05-18_17.38.04_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.75_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 0 percentage 0.0 length_nonzero 12544\n",
      "pruning 0 percentage 0.0 length_nonzero 160\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=16.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=16.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.0\n",
      "Saved results/2022-05-18_17.38.04_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.75_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 784 percentage 0.0625 length_nonzero 12544\n",
      "pruning 10 percentage 0.0625 length_nonzero 160\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=15.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=15.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.0625\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 0 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.0976562  |  2.3816392   |  2.2485737  | 0.1073759  |    0.7656250    |   0.7656250   |  0.1883381  |       12.9116275       |    0.7593006    |      12055.0000000      |      6.7904372      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    247808.0000000    |     0.0203321     |\n",
      "Training... 117/118\n",
      "\n",
      "plotting..\n",
      "finished plotting\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 1 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.8984375  |  0.3128059   |  0.3157284  | 0.9081457  |    0.7656250    |   0.7656250   |  0.8308176  |       12.9116275       |    2.9212756    |      12055.0000000      |      8.8659841      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    247808.0000000    |     0.0075746     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 2 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9296875  |  0.2496427   |  0.2538415  | 0.9252298  |    0.7656250    |   0.7656250   |  0.8378946  |       12.9116275       |    1.9189088    |      12055.0000000      |      9.1651855      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    247808.0000000    |     0.0076564     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "SAVING...\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 3 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9375000  |  0.2099359   |  0.2292394  | 0.9329102  |    0.7656250    |   0.7656250   |  0.8410298  |       12.9116275       |    2.5065125    |      12055.0000000      |      9.3406655      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    247808.0000000    |     0.0077435     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 4 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9492188  |  0.1928940   |  0.2127780  | 0.9397346  |    0.7656250    |   0.7656250   |  0.8437919  |       12.9116275       |    2.6259522    |      12055.0000000      |      9.4652983      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    247808.0000000    |     0.0076540     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "SAVING...\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 5 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9335938  |  0.1691121   |  0.2014752  | 0.9426298  |    0.7656250    |   0.7656250   |  0.8449570  |       12.9116275       |    2.2174263    |      12055.0000000      |      9.5620247      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    247808.0000000    |     0.0076368     |\n",
      "Training... 117/118\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 784])\n",
      "torch.Size([15])\n",
      "torch.Size([10, 15])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for key,param in model.named_parameters():\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved models and saved metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP2(\n",
       "  (layers): Sequential(\n",
       "    (0): ContainerLinear(in_features=784, out_features=15.0, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): ContainerLinear(in_features=15.0, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_path = \"gitignored/results\"\n",
    "ex_paths = os.listdir(result_path)\n",
    "\n",
    "models_path = os.path.join(result_path,ex_paths[0],\"models\")\n",
    "model_path=os.listdir(models_path)\n",
    "#print(model_path)\n",
    "# read metric\n",
    "metric_name = list(filter(lambda x: \"finished\" in x and \"Metrics\" in x ,model_path))[0]\n",
    "metric_path = os.path.join(models_path,metric_name)\n",
    "\n",
    "with open(metric_path, 'rb') as pickle_file:\n",
    "    metric = pickle.load(pickle_file)\n",
    "metric[\"_data\"][\"acc/test\"]\n",
    "\n",
    "# read model\n",
    "model_name = list(filter(lambda x: \"finished\" in x and \"mod\" in x ,model_path))[0]\n",
    "model_path = os.path.join(models_path,model_name)\n",
    "\n",
    "with open(model_path, 'rb') as pickle_file:\n",
    "    model = pickle.load(pickle_file)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/2022-05-05_10.07.09_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.5_train-scheme=DefaultTrainer_seed=1234/models/MLP2_finished not loaded because file is missing\n",
      "results/2022-05-05_10.07.09_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.5_train-scheme=DefaultTrainer_seed=1234/models/MLP2_mod_finished not loaded because file is missing\n"
     ]
    }
   ],
   "source": [
    "# use DATAMANAGER is better\n",
    "# already has RESULTS_DIR, MODELS_DIR\n",
    "checkpoint_name = \"2022-05-05_10.07.09_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.5_train-scheme=DefaultTrainer_seed=1234\"\n",
    "checkpoint_model_state = \"MLP2_finished\"\n",
    "checkpoint_model = \"MLP2_mod_finished\"\n",
    "state_path = os.path.join(RESULTS_DIR,checkpoint_name,MODELS_DIR,checkpoint_model_state)\n",
    "model_path = os.path.join(RESULTS_DIR,checkpoint_name,MODELS_DIR,checkpoint_model)\n",
    "state= DATA_MANAGER.load_python_obj(state_path)\n",
    "model=DATA_MANAGER.load_python_obj(model_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/nfs/homedirs/wangxun/robustness/SparseNetwork-Verification/gitignored/results/MLP/try/try.pickle.pickle'\n",
      "Failed saving /nfs/homedirs/wangxun/robustness/SparseNetwork-Verification/gitignored/results/MLP/try/try.pickle, continue anyway\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "#save_path='/nfs/homedirs/wangxun/robustness/SparseNetwork-Verification/gitignored/results/MLP/2022-05-05_14.51.27_model=MLP2_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.75_train-scheme=DefaultTrainer_seed=1234/models/MLP2_finished'\n",
    "save_path = '/nfs/homedirs/wangxun/robustness/SparseNetwork-Verification/gitignored/results/MLP/try/try.pickle'\n",
    "DATA_MANAGER.save_python_obj(model,save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ConvertMLP2.fuse of ConvertMLP2(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=15, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=15, out_features=10, bias=True)\n",
       "  )\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change to cpu and eval mode\n",
    "model.to(\"cpu\").eval()\n",
    "\n",
    "# get the model parameters\n",
    "state = model.state_dict()\n",
    "# convet the model to a quantizable model\n",
    "q_model= ConvertMLP2(model)\n",
    "# load previous parameter\n",
    "q_model.load_state_dict(state)\n",
    "# set quantization config\n",
    "q_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "# fuse_model\n",
    "q_model.fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -4.2574,   0.0325,   4.5682,  ...,   0.5100,   1.8329,  -1.0362],\n",
       "        [ -1.2456,  -4.7140,  -3.3629,  ...,  -7.0423,   4.4800,   2.9256],\n",
       "        [  9.0897, -12.7721,  -0.9435,  ...,  -1.4990,   3.4533,   2.1567],\n",
       "        ...,\n",
       "        [ -8.3921,   0.4202,  -7.7041,  ...,   0.1281,   3.2470,   4.2118],\n",
       "        [ -5.7385,  -2.1109,  -6.3270,  ...,   7.6626,  -2.4278,   6.2139],\n",
       "        [ -1.2479,  -7.8603,  -2.0022,  ...,  -1.6067,  -4.2694,   1.1689]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use calibrate date to calibrate model\n",
    "model_prepared = torch.quantization.prepare(q_model)\n",
    "cali_data,label=next(iter(train_loader))\n",
    "model_prepared(cali_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to int8\n",
    "model_int8 = torch.quantization.convert(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvertMLP2(\n",
       "  (layers): Sequential(\n",
       "    (0): QuantizedLinear(in_features=784, out_features=15, scale=0.535760760307312, zero_point=45, qscheme=torch.per_channel_affine)\n",
       "    (1): ReLU()\n",
       "    (2): QuantizedLinear(in_features=15, out_features=10, scale=0.3231096565723419, zero_point=67, qscheme=torch.per_channel_affine)\n",
       "  )\n",
       "  (quant): Quantize(scale=tensor([0.0254]), zero_point=tensor([16]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the model size\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32model  \t Size (KB): 49.247\n",
      "model:  quantized  \t Size (KB): 16.699\n"
     ]
    }
   ],
   "source": [
    "size_model_fp32 = print_size_of_model(model,label=\"fp32model\")\n",
    "size_model_int8 = print_size_of_model(model_int8,label=\"quantized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manually implement inference of quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cudnn/Conv.cpp#L304 \n",
    "# for detaile| numeric implementation\n",
    "# get model\n",
    "\n",
    "images,labels=next(iter(test_loader))\n",
    "#torch.max(torch.clamp((torch.round(images/x_1_scale+16)),min=0,max=127)-model_int8.quant(images).int_repr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the fixed parameters in the model\n",
    "W_scales=[]\n",
    "\n",
    "W_int=[]\n",
    "B=[]\n",
    "out_scales=[]\n",
    "out_zps=[]\n",
    "\n",
    "in_scale= model_int8.quant.scale\n",
    "in_zp= torch.tensor(model_int8.quant.zero_point,dtype=torch.int8)\n",
    "\n",
    "for layer in model_int8.layers:\n",
    "    if hasattr(layer,\"scale\"):\n",
    "        w,b = layer._weight_bias()\n",
    "        W_scales.append(w.q_per_channel_scales().numpy())\n",
    "        W_int.append(w.int_repr())\n",
    "        B.append(b.detach())\n",
    "        out_scales.append(layer.scale)\n",
    "        out_zps.append(layer.zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_int8(x):\n",
    "    #return torch.clamp(torch.round(x),min=0,max=127)\n",
    "    return torch.clamp(torch.round(x),min=0)\n",
    "def relu_int8(x,zero_point):\n",
    "    return (x<=zero_point)*zero_point+(x>zero_point)*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "images,labels=next(iter(test_loader))\n",
    "# convert the input to int\n",
    "images = images.squeeze().view(-1,28*28)\n",
    "\n",
    "x_int=round_int8(images/in_scale+in_zp)\n",
    "n_layers = len(W_int)\n",
    "# calculate output for every layer \n",
    "_output=[]\n",
    "\n",
    "for i in range(n_layers):\n",
    "    # formula to calculate output\n",
    "    out=(x_int-in_zp).int()@W_int[i].T.int()*W_scales[i]*in_scale/out_scales[i]+B[i]/out_scales[i]+out_zps[i]\n",
    "    out=round_int8(out)\n",
    "    _output.append(out)\n",
    "    # layers with relu activation\n",
    "    if i<n_layers-1:\n",
    "    # calculate ReLU activation\n",
    "        out = relu_int8(out,out_zps[i])\n",
    "        in_zp = out_zps[i]\n",
    "        in_scale = out_scales[i]\n",
    "        x_int=out\n",
    "        _output.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# test if match the model output\n",
    "print(torch.sum(torch.argmax(out,axis=1)!=torch.argmax(model_int8(images),axis=1)))\n",
    "#print(_output[0][10])\n",
    "#print(_output[1][10])\n",
    "#print(_output[2][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label is  0\n"
     ]
    }
   ],
   "source": [
    "# get a test image \n",
    "images,labels=next(iter(test_loader))\n",
    "img = images[10]\n",
    "label = labels[10]\n",
    "print(\"label is \",label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# target output for every layer\n",
    "#input_1=model_int8.quant(img).view(-1,28*28)\n",
    "#input_1 = torch.round(img/0.0254+16).view(-1,28*28)\n",
    "input_1 = model_int8.quant(images).view(-1,28*28)\n",
    "out_1=model_int8.layers[0](input_1)\n",
    "out_2=model_int8.layers[1](out_1)\n",
    "out_3=model_int8.layers[2](out_2)\n",
    "out_4 = model_int8.dequant(out_3)\n",
    "#print(out_1)\n",
    "#print(out_2)\n",
    "#print(out_3)\n",
    "#print(out_4)\n",
    "#print(out_1.int_repr())\n",
    "#print(out_2.int_repr())\n",
    "#print(out_3.int_repr())\n",
    "#print(torch.max(input_1.int_repr()))\n",
    "#print(torch.max(out_1.int_repr()))\n",
    "#print(torch.max(out_2.int_repr()))\n",
    "#print(torch.max(out_3.int_repr()))\n",
    "print(torch.sum(out_1.int_repr()!=_output[0]))\n",
    "print(torch.sum(out_2.int_repr()!=_output[1]))\n",
    "print(torch.sum(out_3.int_repr()!=_output[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to verify a quantized model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the fixed parameters of the model\n",
    "W_scales=[]\n",
    "#W_zps=0 so remove here\n",
    "#W_zps=[]\n",
    "W_int=[]\n",
    "B=[]\n",
    "out_scales=[]\n",
    "out_zps=[]\n",
    "\n",
    "in_scale= model_int8.quant.scale.numpy()\n",
    "in_zp= torch.tensor(model_int8.quant.zero_point,dtype=torch.int8).numpy()\n",
    "\n",
    "for layer in model_int8.layers:\n",
    "    if hasattr(layer,\"scale\"):\n",
    "        w,b = layer._weight_bias()\n",
    "        W_scales.append(w.q_per_channel_scales().numpy())\n",
    "        W_int.append(w.int_repr().numpy())\n",
    "        B.append(b.detach().numpy().reshape(-1))\n",
    "        out_scales.append(layer.scale)\n",
    "        out_zps.append(layer.zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label is  7\n"
     ]
    }
   ],
   "source": [
    "# get a test img\n",
    "images,labels=next(iter(test_loader))\n",
    "img_test = images[0]\n",
    "num_classes=model_int8.output_dim\n",
    "c = torch.argmax(model_int8(img_test)).item()\n",
    "print(\"label is \",c)\n",
    "other_classes = set(range(num_classes))\n",
    "other_classes.remove(c)\n",
    "\n",
    "# perturbation budget\n",
    "eps=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1=W_int[0]\n",
    "b1=B[0]\n",
    "W2=W_int[1]\n",
    "b2=B[1]\n",
    "img_test=img_test.numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lower and upper bounds before first quantization \n",
    "u0=img_test+eps\n",
    "l0=img_test-eps\n",
    "# step 1 quantize x\n",
    "# _x = x/in_scale + in_zp\n",
    "# _x_int = round(_x)\n",
    "# x_int = clamp(_x_int,min=0)\n",
    "x = cp.Variable(shape=(img_test.shape))\n",
    "_x = cp.Variable(shape=(img_test.shape))\n",
    "_x_int = cp.Variable(shape=(img_test.shape),integer=True)\n",
    "x_int = cp.Variable(shape=(img_test.shape),integer=True)\n",
    "# boolean variable for clamp constraint\n",
    "a0 = cp.Variable(shape=(img_test.shape), boolean=True)\n",
    "\n",
    "\n",
    "# output of first layer\n",
    "# y1 = W1@(x_int-x_zp)*W_scales[0]*in_scale/out_scales[0]+b1/out_scales[0]+out_zps[0]\n",
    "# _y1_int = round(y1)\n",
    "# y1_int = clamp(_y1_int,min=0)\n",
    "y1 = cp.Variable(shape=(W1.shape[0]))\n",
    "_y1_int = cp.Variable(shape=(W1.shape[0]),integer=True)\n",
    "y1_int = cp.Variable(shape=(W1.shape[0]),integer=True)\n",
    "# boolean variable for clamp constraints\n",
    "a1_1 = cp.Variable(shape=(W1.shape[0]), boolean=True)\n",
    "\n",
    "# output of a ReLU function\n",
    "y1_out = cp.Variable(shape=(W1.shape[0]),integer=True)\n",
    "#  boolean variable for relu constraints\n",
    "a1_2 = cp.Variable(shape=(W1.shape[0]), boolean=True)\n",
    "\n",
    "# output of second layer\n",
    "# y2 = W2@(y1_out-out_zps[0])*W_scales[1]*out_scales[0]/out_scales[1]+b2/out_scales[1]+out_zps[1]\n",
    "# _y2_int = round(y2)\n",
    "# y2_int = clamp(_y2_int,min=0)\n",
    "y2 = cp.Variable(shape=(W2.shape[0]))\n",
    "_y2_int = cp.Variable(shape=(W2.shape[0]),integer=True)\n",
    "y2_int = cp.Variable(shape=(W2.shape[0]),integer=True)\n",
    "#  boolean variable for clamp constraints\n",
    "a2 = cp.Variable(shape=(W2.shape[0]), boolean=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lower and upper bounds after scaling\n",
    "in_scale_p = np.maximum(0,1/in_scale)\n",
    "in_scale_m = np.maximum(-1/in_scale,0)\n",
    "\n",
    "u0_1 = in_scale_p*u0-in_scale_m*l0+in_zp\n",
    "l0_1 = in_scale_p*l0-in_scale_m*u0+in_zp\n",
    "\n",
    "# get lower and upper bounds after round\n",
    "_u0_int=np.round(u0_1)\n",
    "_l0_int=np.round(l0_1)\n",
    "\n",
    "# get lower and upper bounds after clamp\n",
    "u0_int=np.clip(_u0_int,a_min=0,a_max=None)\n",
    "l0_int=np.clip(_l0_int,a_min=0,a_max=None)\n",
    "\n",
    "# get lower and upper bounds after first layer calculation\n",
    "W1_p = np.maximum(0,W1)\n",
    "W1_m = np.maximum(-W1,0)\n",
    "\n",
    "u1 = (W1_p @ (u0_int-in_zp) - W1_m @ (l0_int-in_zp))*W_scales[0]*in_scale/out_scales[0] + b1/out_scales[0]+out_zps[0]\n",
    "l1 = (W1_p @ (l0_int-in_zp) - W1_m @ (u0_int-in_zp))*W_scales[0]*in_scale/out_scales[0] + b1/out_scales[0]+out_zps[0]\n",
    "\n",
    "# get lower and upper bounds after round\n",
    "_u1_int=np.round(u1)\n",
    "_l1_int=np.round(l1)\n",
    "\n",
    "# get lower and upper bounds after clamp\n",
    "u1_int=np.clip(_u1_int,a_min=0,a_max=None)\n",
    "l1_int=np.clip(_l1_int,a_min=0,a_max=None)\n",
    "\n",
    "# get lower and upper bounds after relu  do I need this ?\n",
    "\n",
    "# get lower and upper bounds after second layer calculation\n",
    "# the output scale and zero point of first layer are the input scale and zero point of second layer\n",
    "W2_p = np.maximum(0,W2)\n",
    "W2_m = np.maximum(-W2,0)\n",
    "\n",
    "u2 = (W2_p @ (u1_int-out_zps[0]) - W2_m @ (l1_int-out_zps[0]))*W_scales[1]*out_scales[0]/out_scales[1] + b2/out_scales[1]+out_zps[1]\n",
    "l2 = (W2_p @ (l1_int-out_zps[0]) - W2_m @ (u1_int-out_zps[0]))*W_scales[1]*out_scales[0]/out_scales[1] + b2/out_scales[1]+out_zps[1]\n",
    "\n",
    "# get lower and upper bounds after round\n",
    "_u2_int=np.round(u1)\n",
    "_l2_int=np.round(l1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constraints\n",
    "constraints=[]\n",
    "constraints += [_x==x/in_scale+in_zp]\n",
    "# round operation\n",
    "constraints += [cp.atoms.norm_inf(_x-_x_int) <= 0.5]\n",
    "# clamp operation one similar to relu \n",
    "# constrains of clip (x>0)\n",
    "constraints += [x_int[i] <= _x_int[i] - _l0_int[i] * (1 - a0[i]) for i in range(x_int.shape[0])]\n",
    "constraints += [x_int[i] >= _x_int[i] for i in range(x_int.shape[0])]\n",
    "constraints += [x_int[i] <= _u0_int[i]*a0[i] for i in range(x_int.shape[0])]\n",
    "constraints += [x_int[i] >= 0 for i in range(x_int.shape[0])]\n",
    "\"\"\"\n",
    "# not used again\n",
    "# constraints of clip (x<127)\n",
    "constraints += [x_int[i] >= _x_int[i] - (_u0_int[i]-127) * (1 - a0_2[i]) for i in range(x_int.shape[0])]\n",
    "constraints += [x_int[i] <= _x_int[i] for i in range(x_int.shape[0])]\n",
    "constraints += [x_int[i] >= 127+(_l0_int[i]-127)*a0_2[i] for i in range(x_int.shape[0])]\n",
    "constraints += [x_int[i] <= 127 for i in range(x_int.shape[0])]\n",
    "\"\"\"\n",
    "# constraints of first layer \n",
    "constraints += [y1 == W1@(x_int-in_zp)*W_scales[0]*in_scale/out_scales[0]+b1/out_scales[0]+out_zps[0]]\n",
    "\n",
    "# constrains of round operation\n",
    "constraints += [cp.atoms.norm_inf(y1-_y1_int) <= 0.5]\n",
    "\n",
    "# constrains of clip\n",
    "constraints += [y1_int[i] <= _y1_int[i] - _l1_int[i] * (1 - a1_1[i]) for i in range(y1_int.shape[0])]\n",
    "constraints += [y1_int[i] >= _y1_int[i] for i in range(y1_int.shape[0])]\n",
    "constraints += [y1_int[i] <= _u1_int[i]*a1_1[i] for i in range(y1_int.shape[0])]\n",
    "constraints += [y1_int[i] >= 0 for i in range(y1_int.shape[0])]\n",
    "\"\"\"\n",
    "# not used again\n",
    "constraints += [y1_int[i] >= _y1_int[i] - (_u1_int[i]-127) * (1 - a1_2[i]) for i in range(y1_int.shape[0])]\n",
    "constraints += [y1_int[i] <= _y1_int[i] for i in range(y1_int.shape[0])]\n",
    "constraints += [y1_int[i] >= 127+(_l1_int[i]-127)*a1_2[i] for i in range(y1_int.shape[0])]\n",
    "constraints += [y1_int[i] <= 127 for i in range(y1_int.shape[0])]\n",
    "\"\"\"\n",
    "\n",
    "# constraints for relu of first layer (zero_point = out_zps[0])\n",
    "\n",
    "constraints += [y1_out[i] <= y1_int[i] - (l1_int[i]-out_zps[0]) * (1 - a1_2[i]) for i in range(y1_int.shape[0])]\n",
    "constraints += [y1_out[i] >= y1_int[i] for i in range(y1_int.shape[0])]\n",
    "constraints += [y1_out[i] <= out_zps[0]+(u1_int[i]-out_zps[0])*a1_2[i] for i in range(y1_int.shape[0])]\n",
    "constraints += [y1_out[i] >= out_zps[0] for i in range(y1_int.shape[0])]\n",
    "\n",
    "# constraints of second layer\n",
    "constraints += [y2 == W2@(y1_out-out_zps[0])*W_scales[1]*out_scales[0]/out_scales[1]+b2/out_scales[1]+out_zps[1]]\n",
    "\n",
    "# constraints of round operation\n",
    "constraints += [cp.atoms.norm_inf(y2-_y2_int) <= 0.5]\n",
    "\n",
    "# constraints of clip\n",
    "constraints += [y2_int[i] <= _y2_int[i] - _l2_int[i] * (1 - a2[i]) for i in range(y2_int.shape[0])]\n",
    "constraints += [y2_int[i] >= _y2_int[i] for i in range(y2_int.shape[0])]\n",
    "constraints += [y2_int[i] <= _u2_int[i]*a2[i] for i in range(y2_int.shape[0])]\n",
    "constraints += [y2_int[i] >= 0 for i in range(y2_int.shape[0])]\n",
    "\n",
    "# constrainst of input pertubation\n",
    "constraints += [cp.atoms.norm_inf(img_test-x) <= eps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b32b3db1334921a69728e556139ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for other in tqdm(other_classes):\n",
    "    problem=cp.Problem(objective=cp.Minimize(y2_int[c]-y2_int[other]),constraints=constraints)\n",
    "    opt = problem.solve('MOSEK')\n",
    "    if opt < 0:\n",
    "        print('found solution')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-7e1290843896>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"unperturbed prediction: {c}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m122\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"perturbed prediction: {y2_int.value.argmax()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reshape'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAasklEQVR4nO3de7hcVZ3m8e9LAG+hEUygYwgEMSDRUcAj0g8whge0Q1RCNw4D7QU0Q2wHHC8M04gX6NgXtFsceRq1YxNBG4F4z2AEncilVcKQKGBCRBPkEkggEIQgIkTf+WPv41M5VSencup2zq738zz1ZNfaa9f67UryO+usvfdask1ERFTPTr0OICIiOiMJPiKiopLgIyIqKgk+IqKikuAjIioqCT4ioqKS4KOvSFok6WFJq4bZL0kXS1or6Q5Jh3U7xoh2SYKPfnMZMHs7+48HZpSv+cDnuhBTREckwUdfsX0TsHk7VeYCX3JhOfBCSVO6E11Ee+3c6wAixpipwP0179eXZRuGVpQ0n6KXzwte8IJXv+xlL+tKgNF/Vq5c+YjtyTt6XBJ8xCjZXggsBBgYGPCKFSt6HFFUlaR7R3NchmgitvUAMK3m/T5lWcS4kwQfsa0lwDvKu2mOAB63XTc8EzEeZIgm+oqkK4FZwCRJ64HzgV0AbH8eWArMAdYCTwHv7E2kEa1Lgo++YvvUEfYbOLNL4UR0VIZoIiIqKgk+IqKikuAjIioqCT4ioqKS4CMiKioJPiKiopLgIyIqKgk+IqKikuAjIioqCT4ioqKS4CMiKioJPiKiopLgWyBpVjkjYbs+z5Je2q7P28G2b5D038rtt0r63ig/57uSTmtvdBExGn2b4CVdJunveh3HWGT7CttvGKmepAsk/fuQY4+3fXnnovtj29+V9GTN6xlJP+t0uxHjSV9OFyxpQhs+Y8x+d5J2tr2113F0ku3ja99LugH4QW+iiRibutqDHzoEUduLHhzukHS2pIclbZD0ziF1Py/p+5K2SLpR0n41+19W7tss6S5JJw859nOSlkr6DTAPeCvwv8re3//Zgfj+RtJG4Is19c6T9IikeyS9tab8OZL+WdJ9kh4q439ezf5zyvN8UNK7RvjubpD0j5L+n6QnJH1b0p7lvull7PMk3UeZ6CS9S9IaSY9Jum7I9/V6ST+X9LikfwFUs+90ST+sef/ymu/2ofJ8ZwPnAf+1/A5vr4lzcKhnJ0kfkXRv+Xf6JUm7D4n5tPL7eUTSh7f3HWznu5kOHA18aTTHR1TVWBui+VNgd4pV7OcBl0jao2b/W4GPA5OA24ArACS9APg+8BVgL+AU4LOSZtYc+1fA3wO7USSCK4BP2p5o+807EN+ewH7A/JqySWXMpwELJR1U7rsQOBA4BHhpWedjZcyzgf8JvB6YARzXRPvvAN4FTAG2AhcP2f864GDgzyXNpUjAfwlMBv4DuLJsexLwDeAjZezrgCMbNShpN+D/AtcCLy7PY5nta4F/AK4uv8NXNTj89PJ1DPASYCLwL0PqHAUcBBwLfEzSwWW7R0n69Qjfx6B3AP9h+54m60f0hbGW4J8FFth+1vZS4EmK//yDvmP7Jtu/Az4M/JmkacCbgHtsf9H2Vts/Bb4O/JeaY79t+0e2/2D76VHG9wfgfNu/s/3bmvKPlmU3At8BTpYkih8CH7C92fYWioR4SnnMycAXba+y/Rvggiba/3JN/Y+W7dQON11g+zdlbH8N/KPtNeVwzT8Ah5S9+DnAattfs/0s8L+BjcO0+SZgo+1P2X7a9hbbtzQRKxQ/kC+yfbftJ4EPAacMGd76W9u/tX07cDvwKgDbP7T9wibbeQdwWZN1I/rGWBtHfnTI2PFTFL2+QfcPbth+UtJmil7lfsBrh/T4dga+3OjYFmxq8MPhsTLhDrq3jGky8HxgZZHrgWIYZDAhvxhYOeS4kdSew70Ua4lOGmb/fsBnJH2qpkwUv0W8mG2/S0sa7vuZRtHDH40Xs+153Uvx97J3TVntD5ahf98jknQUxW9RXxtljBGV1e0e/FMUSW/Qn+7g8dMGNyRNpBgueZAiWd1o+4U1r4m231NzrId81tD3zcTX6Jg9yiGiQfuWMT0C/BZ4eU1Mu9seTGAbas+nPG4kQ+s/W7bTKL77gXcP+U6eZ/vHQ9suf9uo/WyGfM5LhtnX6Puo9SDFD5ramLcCD41w3I44DfhG+RtCRNTodoK/DfgrSRPKMejX7eDxc8qx2V0pxuKX274fuAY4UNLbJe1Svl4zOJ47jIeoT1yjje9vJe0q6WiKIY2v2v4D8AXg05L2ApA0VdKfl8csBk6XNFPS84Hzm2jnbTX1FwBfs/37Yep+HviQpJeXbe8uaXDI6jvAyyX9ZTlc8j8Y/oftNcAUSe8vLxrvJum15b6HgOmShvt3dCXwAUn7lz+QB8fs23KHT3nB+mQyPBPRULcT/PuANwO/phif/dYOHv8VikS4GXg18DaAcnz7DRTj2w9S/Nr/CeA52/msS4GZkn4taTCO0cS3EXisbPcK4K9t/7zc9zfAWmC5pCcoLlYeVMb8XYqx7x+UdZq5xe/LFMlsI/BcisTckO1vUnwHV5VtrwKOL/c9QnF94kLgUYqLvD8a5nO2UFwIfnPZ7i8pLpoCfLX881FJP2lw+KIy5puAXwFPA+9t4jyRdLSkkXrlJ1L8XV3fzGdG9BvZI/2WPTZIugxYb/sjvY6lF1Tc5/3vtv+t17FEvYGBAa9YsaLXYURFSVppe2BHjxtrd9FERESbJMFHRFTUWLtNcli2T+91DL1ke1avY4iI8SU9+IiIimqpB1/eSvgZiod3/s32hSPUHx9XdGPcsq2Ra0X0h1H34MtH5C+huPVuJnDqkLlfIiKih1oZojkcWFvOM/IMcBUwtz1hRUREq1pJ8FPZdu6T9WXZNiTNl7RCUm4Sjojooo7fRWN7IbAQMgYfEdFNrfTgH2DbCar2KcsiImIMaCXB3wrMKCeS2pViHpgl7QkrIiJaNeohGttbJZ0FXEdxm+Qi26vbFllERLSkpQedbC+1faDtA2z/fbuCiugUSbNVrNm7VtK5DfbvK+l6ST+VdIekOb2IM6Id8iRr9I0mn934CLDY9qGUa/t2N8qI9kmCj37SzLMbBv6k3N6dYp7/iHEpCT76STPPblxAsXLWemAp21mgpPYZj02bNrU71oiWJcFHbOtU4DLb+wBzgC8PtySh7YW2B2wPTJ48uatBRjQjCT76STPPbsyjWC8X2zdTLI04qSvRRbRZEnz0k2ae3bgPOBagXLT9uUDGX2JcSoKPvmF7KzD47MYairtlVktaIOmEstrZwBmSbgeuBE73eFm4OGKIcbOiU0Q72F5KcfG0tuxjNdt3Akd2O66ITkgPPiKiopLgIyIqKgk+IqKikuAjIioqCT4ioqKS4CMiKioJPiKiopLgIyIqKgk+IqKikuAjIiqqpakKJN0DbAF+D2y1PdCOoCIionXtmIvmGNuPtOFzIiKijTJEExFRUa0meAPfk7RS0vxGFWqXNWuxrYiI2AGtDtEcZfsBSXsB35f0c9s31VawvRBYCCAp82pHRHRJSz142w+Ufz4MfJNi1fqIiBgDRp3gJb1A0m6D28AbgFXtCiwiIlrTyhDN3sA3JQ1+zldsX9uWqCIiomWjTvC27wZe1cZYIiKijXKbZERERfXdottvectb6srOOOOMhnUffPDBurKnn366Yd0rrriirmzjxo0N665du3Z7IUZEtEV68BERFZUEHxFRUUnwEREVlQQfEVFRSfARERUlu3vTw4yFuWjuvvvuurLp06d3pK0tW7Y0LF+9enVH2uuE9evXNyz/5Cc/WVe2YkXv55OzrV60OzAw4LFw/lFNklaOZr2N9OAjIioqCT4ioqKS4CMiKioJPiKiovpuqoJG0xK88pWvbFh3zZo1dWUHH3xww7qHHXZYXdmsWbMa1j3iiCPqyu6///66smnTpjU8fkds3bq1rmzTpk0N606ZMqXpz73vvvvqynKRMWJsSQ8+IqKikuAjIioqCT76iqTZku6StFbSucPUOVnSnZJWS/pKt2OMaJe+G4OP/iVpAnAJ8HpgPXCrpCW276ypMwP4EHCk7cfKBeUjxqX04KOfHA6stX237WeAq4C5Q+qcAVxi+zH444LyEePSiD14SYuANwEP235FWbYncDUwHbgHOHnwP8RYt2zZsqbKhnPttc0vO7vHHns0LD/kkEPqylauXFlX9prXvKbptobTaIGSX/ziFw3rNrpraM8992xYd926da0F1htTgdrbldYDrx1S50AAST8CJgAXDLfWsKT5wHyAfffdt+3BRrSqmR78ZcDsIWXnAstszwCWle8jqmBnYAYwCzgV+IKkFzaqaHuh7QHbA5MnT+5ehBFNGjHB274J2DykeC5webl9OXBie8OK6IgHgNqHC/Ypy2qtB5bYftb2r4BfUCT8iHFntGPwe9veUG5vBPYerqKk+ZJWSMpTMNFrtwIzJO0vaVfgFGDJkDrfoui9I2kSxZBN/RSkEeNAyxdZXcw3POw0wLW/xrbaVkQrbG8FzgKuA9YAi22vlrRA0gllteuARyXdCVwPnGP70d5EHNGapuaDlzQduKbmIutdwCzbGyRNAW6wfVATn9Pz+eADTjrppIblixcvritbtWpVw7rHHHNMXdnmzUNH8rov88FHFXV7PvglwGnl9mnAt0f5ORER0SEjJnhJVwI3AwdJWi9pHnAh8HpJvwSOK99HRMQYMuJ98LZPHWbXsW2OJSIi2ihPskZEVFQSfERERWWysYrba6/6ubI++9nPNqy70071P+8XLFjQsO5YuGMmIrYvPfiIiIpKgo+IqKgk+IiIikqCj4ioqFxkrbgzzzyzrmy4qW0fe6x+Sv+77rqr7TFFRHekBx8RUVFJ8BERFZUEHxFRUUnwEREVlYusFXHkkUc2LD/33OaXyz3xxBPryoabDz4ixr704CMiKioJPiKiopLgIyIqKgk+IqKikuAjIipqxLtoJC0C3gQ8bPsVZdkFwBnAprLaebaXdirIGNmcOXMalu+yyy51ZcuWLWtY9+abb25rTBHRW8304C8DZjco/7TtQ8pXkntExBgzYoK3fROQ5XsiIsaZVsbgz5J0h6RFkvYYrpKk+ZJWSFrRQlsREbGDRpvgPwccABwCbAA+NVxF2wttD9geGGVbERExCqOaqsD2Q4Pbkr4AXNO2iGJEz3ve8+rKZs9udJkEnnnmmbqy888/v2HdZ599trXAImJMGVUPXtKUmrd/AWTCkoiIMaaZ2ySvBGYBkyStB84HZkk6BDBwD/DuzoUYERGjMWKCt31qg+JLOxBLRES0UZ5kjYioqCT4iIiKyoIf49A555xTV3booYc2rHvttdfWlf34xz9ue0wRMfakBx8RUVFJ8BERFZUEH31F0mxJd0laK2nYBWslnSTJkvIEdoxbSfDRNyRNAC4BjgdmAqdKmtmg3m7A+4BbuhthRHvlIusY9sY3vrFh+Uc/+tG6sieeeKJh3QULFrQ1pnHucGCt7bsBJF0FzAXuHFLv48AngPqr2RHjSHrw0U+mAvfXvF9flv2RpMOAaba/M9KH1c6UumnTppGqR3RdEnxESdJOwEXA2c3Ur50pdfLkyZ0NLmIUkuCjnzwATKt5v09ZNmg34BXADZLuAY4AluRCa4xXSfDRT24FZkjaX9KuwCnAksGdth+3Pcn2dNvTgeXACbazWE2MS0nw0TdsbwXOAq4D1gCLba+WtEDSCb2NLqL9chfNGPGiF72oruziiy9uWHfChAl1ZUuXNl73fPny5a0FVjHlAvFLh5R9bJi6s7oRU0SnpAcfEVFRSfARERWVBB8RUVFJ8BERFdXMmqzTgC8Be1OswbrQ9mck7QlcDUynWJf1ZNuPdS7U6mh0kbTRvO37779/w+PXrVtXV9Zo+oKI6G/N9OC3Amfbnknx4MeZ5QRN5wLLbM8AlpXvIyJijBgxwdveYPsn5fYWivuHp1JM0nR5We1y4MQOxRgREaOwQ/fBS5oOHEoxjeretjeUuzZSDOE0OmY+ML+FGCMiYhSavsgqaSLwdeD9treZm9a2Kcbn69ROyNRSpBERsUOaSvCSdqFI7lfY/kZZ/JCkKeX+KcDDnQkxIiJGo5m7aARcCqyxfVHNriXAacCF5Z/f7kiEFXTAAQfUlb361a9u+vgPfvCDdWWN7qyJiP7WzBj8kcDbgZ9Juq0sO48isS+WNA+4Fzi5IxFGRMSojJjgbf8Q0DC7j21vOBER0S55kjUioqKS4CMiKirzwXfQfvvt17D8e9/7XlPHn3POOQ3Lr7nmmlHHFBH9Iz34iIiKSoKPiKioJPiIiIpKgo+IqKgk+IiIispdNB00f37jSTT33Xffpo6/8cYbG5YXc7tFRGxfevARERWVBB8RUVFJ8BERFZUEHxFRUbnI2iZHHXVUXdl73/veHkQSEVFIDz4ioqKS4CMiKioJPiKiopLgIyIqasQEL2mapOsl3SlptaT3leUXSHpA0m3la07nw42IiGY1cxfNVuBs2z+RtBuwUtL3y32ftv3PnQtv/Dj66KPryiZOnNj08evWrasre/LJJ1uKKSL6WzOLbm8ANpTbWyStAaZ2OrCIiGjNDo3BS5oOHArcUhadJekOSYsk7dHu4CLaTdJsSXdJWivp3Ab7P1gOR94haZmkxusuRowDTSd4SROBrwPvt/0E8DngAOAQih7+p4Y5br6kFZJWtB5uxOhJmgBcAhwPzAROlTRzSLWfAgO2Xwl8Dfhkd6OMaJ+mErykXSiS+xW2vwFg+yHbv7f9B+ALwOGNjrW90PaA7YF2BR0xSocDa23fbfsZ4Cpgbm0F29fbfqp8uxzYp8sxRrTNiGPwkgRcCqyxfVFN+ZRyfB7gL4BVnQmxem6//fa6smOPPbaubPPmzd0Ip59MBe6veb8eeO126s8DvjvcTknzgfnQ/Bz/Ed3UzF00RwJvB34m6bay7DyKX28PAQzcA7y7A/FF9ISktwEDwOuGq2N7IbAQYGBgIKuwxJjTzF00PwTUYNfS9ocT0VEPANNq3u9Tlm1D0nHAh4HX2f5dl2KLaLs8yRr95FZghqT9Je0KnAIsqa0g6VDgX4ETbD/cgxgj2iYJPvqG7a3AWcB1wBpgse3VkhZIOqGs9k/AROCr5RPaS4b5uIgxL/PBR1+xvZQhw4u2P1azfVzXg4roENnduzYkKReioqNsN7pe1HEDAwNesSKPekRnSFo5mlvNM0QTEVFRSfARERWVBB8RUVFJ8BERFdXtu2geAe4ttyeV76sm59U7mfkxokZXE7ztyYPbklZUcQKynFdEjBUZoomIqKgk+IiIiuplgl/Yw7Y7KecVEWNCzxJ8OdVq5eS8ImKsyBBNRERFJcFHRFRU1xP8SKvajyeSFkl6WNKqmrI9JX1f0i/LP/foZYyjIWmapOsl3SlptaT3leXj/twi+klXE3yTq9qPJ5cBs4eUnQsssz0DWFa+H2+2AmfbngkcAZxZ/j1V4dwi+ka3e/Ajrmo/nti+CRi6MvZc4PJy+3LgxG7G1A62N9j+Sbm9hWJxjKlU4Nwi+km3E3yjVe2ndjmGTtvb9oZyeyOwdy+DaZWk6cChwC1U7Nwiqi4XWTvIxWoq43aRE0kTga8D77f9RO2+8X5uEf2g2wm+qVXtx7mHJE0BKP8clws3S9qFIrlfYfsbZXElzi2iX3Q7wY+4qn0FLAFOK7dPA77dw1hGRZKAS4E1ti+q2TXuzy2in3R7NsmtkgZXtZ8ALLK9upsxtJOkK4FZwCRJ64HzgQuBxZLmUUyNfHLvIhy1I4G3Az+TdFtZdh7VOLeIvtHVRbcjqiqLbkcnZdHtiIjYRhJ8RERFJcFHRFRUEnxEREUlwUdEVFQSfERERSXBR0RUVBJ8RERFJcFHRFRUEnxEREUlwUdEVFQSfERERSXBR0RUVBJ89BVJsyXdJWmtpLpFwyU9R9LV5f5byiULI8alJPjoG5ImAJcAxwMzgVMlzRxSbR7wmO2XAp8GPtHdKCPaJwk++snhwFrbd9t+BrgKmDukzlzg8nL7a8Cx5QpXEeNOV1d0iuixqcD9Ne/XA68drk65AtnjwIuAR4Z+mKT5wPzy7e8krWp7xCObRIPYKtxuL9vu5TkfNJqDkuAjRsn2QmAhgKQVo1lxp1X91m4v2+71OY/muAzRRD95AJhW836fsqxhHUk7A7sDj3Yluog2S4KPfnIrMEPS/pJ2BU4BlgypswQ4rdx+C/ADZ+HiGKcyRBN9oxxTPwu4DpgALLK9WtICYIXtJcClwJclrQU2U/wQaMbCjgSddsdS2+PunJXOSURENWWIJiKiopLgIyIqKgk+okm9nOagibY/KOlOSXdIWiZpv260W1PvJEmW1LbbCJtpW9LJ5XmvlvSVbrQraV9J10v6afl9z2lTu4skPTzc8xQqXFzGdYekw0b8UNt55ZXXCC+Ki7LrgJcAuwK3AzOH1PnvwOfL7VOAq7vY9jHA88vt97Sj7WbaLevtBtwELAcGunjOM4CfAnuU7/fqUrsLgfeU2zOBe9p0zv8ZOAxYNcz+OcB3AQFHALeM9JnpwUc0p5fTHIzYtu3rbT9Vvl1OcY9/x9stfZxizp6n29DmjrR9BnCJ7ccAbD/cpXYN/Em5vTvwYBvaxfZNFHduDWcu8CUXlgMvlDRle5+ZBB/RnEbTHEwdro7trcDgNAfdaLvWPIqeXsfbLYcJptn+Thva26G2gQOBAyX9SNJySbO71O4FwNskrQeWAu9tQ7vN2NF/B7kPPqJKJL0NGABe14W2dgIuAk7vdFvD2JlimGYWxW8sN0n6T7Z/3eF2TwUus/0pSX9G8dzEK2z/ocPt7rD04COa08tpDpppG0nHAR8GTrD9uy60uxvwCuAGSfdQjAsvadOF1mbOeT2wxPaztn8F/IIi4Xe63XnAYgDbNwPPpZiIrNOa+ndQKwk+ojm9nOZgxLYlHQr8K0Vyb8dY9Ijt2n7c9iTb021Ppxj7P8H2qCbG2pG2S9+i6L0jaRLFkM3dXWj3PuDYst2DKRL8phbbbcYS4B3l3TRHAI/b3rC9AzJEE9EEd3aag3a0/U/AROCr5XXd+2yf0IV2O6LJtq8D3iDpTuD3wDm2W/qNqcl2zwa+IOkDFBdcT2/HD3JJV1L8wJpUju+fD+xSxvV5ivH+OcBa4CngnSN+Zns6GBERMdZkiCYioqKS4CMiKioJPiKiopLgIyIqKgk+IqKikuAjIioqCT4ioqL+P1yIlHrK6eF3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(img_test.reshape(28,28), cmap='gray')\n",
    "plt.title(f\"unperturbed prediction: {c}\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(x.value.reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"perturbed prediction: {y2_int.value.argmax()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directly load model and verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.reshape(-1,1).shape\n",
    "img."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test single image verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label=next(iter(test_loader))\n",
    "img_test = img[0].squeeze().to(device)\n",
    "model.to(device)\n",
    "img,label,indicator=verify_single_image(model=model,image=img_test,eps=0.3)\n",
    "if indicator==0:\n",
    "    print(\"verified!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_test.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAViUlEQVR4nO3de5BcZZnH8e8vYUYQKCsBCZEEUASsIMsti7suLmwJ4VoGKUvIH1bwFmEVdVdYEd01rkYuoqwslGyECHjBCxHBLO7KRRZxCyXhEkJYMYREcmECBAQDtZkwz/5xzrjNMH160qdPnw7v71PVNT3nOZenz/Qz5/R5+z2vIgIze/UbV3cCZtYdLnazRLjYzRLhYjdLhIvdLBEudrNEuNh7nKS5kr7ToXUdJWlNJ9bV5vZD0pvz51dI+sc21/NHSW/qbHavfi72CklaJenouvPoRRFxRkR8sdV8ku6Q9KERy+4UESury+5l2z9N0sOSNkl6VNI7urHdKmxXdwKvRpK2i4gtnVhPJ/KpQqdeYy+TdAxwIXAq8Btgcr0ZleMje4P8SPwZScslPSPpW5K2b4ifJOl+Sc9K+m9JfzZi2U9LWgpsknQdsCfw0/y08x9GO41uPPrnp+zXS/qOpOeA0/PZtpf0A0nPS7pX0kENy79B0kJJT0p6TNLHG2I7SLo6fy3LgT9v8fpD0sclrZT0lKSvSBqXx06X9CtJl0h6Gpgr6TWSLpb0e0kD+an5Dg3rO0fSeknrJH1gxLaulvSlht9n5vv2ufwIepykecA7gMvyfXhZQ57DHwdeJ+na/PWvlvS5ETnflef4TL5/ji/aByN8AfjniLg7IoYiYm1ErN2K5XtLRPiRP4BVwDJgKjAR+BXwpTx2CLABeBswHpidz/+ahmXvz5fdoWHa0Q3rPwpYM8o2j86fzwUGgZPJ/hHv0DDtPUAfcDbwWP58HLAE+CegH3gTsBI4Nl/fBcAv89cyNX9tawpefwC/yOffE3gE+FAeOx3YApxFdka4A3AJcFM+/87AT4Hz8/mPAwaAtwI7At/L1//mPH51w749HPgDcEz+mvYA3pLH7hjOYUSew+u5Frgx3/7eec4fbMh5EPhw/jc7E1gHKI+fCyxqsi/GA5vzeVYAa4DLhv+22+Kj9gR66ZEX3hkNv58APJo//wbwxRHz/xY4smHZD4yyvq0t9jtHxOcCdzf8Pg5YT3bEexvw+xHzfwb4Vv58JXBcQ2zOGIq9cf6/BW7Ln5/euC1AwCZgn4Zpfwk8lj9fAFzQENuvoNj/DbikSU5Ni72hIKc1xD4C3NGQ84qG2GvzZXcfw3vhDfm8i8lO33cl++c/r+73abuPnv1MWKPHG56vJvujA+wFzJZ0VkO8vyE+ctlObP8V0yJiKP8oMPxmfIOkZxvmHU92NCefZ+Tr2ZrtN77+kbHXkxXPEknD05Rvf3jbS8a47anAzWPIbaRdyc5wGte9muzMYNgTw08i4oU8153GsO4X85//GhHrASR9Dfgc8Nk2cq2di/2VpjY835PstA+yN/q8iJhXsOzILoQjf99EViAASBpPVjRFy7wsp/zz6JQ8ry1kR9J9m+SzPl/2ofz3PQtyb9xW4/zrGmKNuT1FVhAHxOifY4e3Paxo248D+zSJFXXLfIrsNH0vYHnDdkp/ro6IZ/J/qo3b36a7iPoC3St9VNIUSRPJ/oP/IJ/+TeAMSW9TZkdJJ0rauWBdA2Sfo4c9Qnax7URJfWRHideMIafDJJ2SX53/JPC/wN1kV4ifzy8M7iBpvKS3Shq+EPdD4DOSJkiaQvZ5u5Vz8vmnAp9oeP0vExFDZPvkEkm7AUjaQ9KxDds+XdI0Sa8FPl+wzauA90t6p6Rx+XreksdG7sPGHF7KtzNP0s6S9gL+HujI9xKAbwFnSdpN0gTg74BFHVp317nYX+l7wM/JPu8+CnwJICIWk13ouQx4huyizekt1nU+8Ln86v3ZEfEHss/BV5IdfTaRXfhp5Uay5p9ngPcBp0TEYP5mPwk4mOyi3VP5ul+XL/cFstPax/LX9O0xbmsJ2cXGfycrxGY+TbYf7s5bD24F9geIiJ8B/wLcns9ze7OVRMRvgPeTXfD7A/BfZEdrgK8D78mvpl86yuJnke3HlcBdZH+/Ba1fJkg6T9LPCmb5InAP2T/ph4H7gKIzu542fFXSyJrByC4G3Vp3LnWQFMC+EbGi7lys83xkN0uEi90sET6NN0uEj+xmiehqO3t+Aaipvr6+bqXSUYODg6WWb/W6y6y/6n3ay7mV0ep1lc29qv22ZcsWhoaGNFqsVLFLOo6saWQ8cGVEXFBmfZMmTSqzeG3WrCnXRbzV6y6z/qr3aS/nVmRoaKgwvm7dusJ42dyr2m8DAwNNY22fxuff/rocOB6YBsySNK3d9ZlZtcp8Zj+crJPByojYDHwfmNmZtMys08oU+x68vGPEGl7eAQEASXMkLZa0uMS2zKykyi/QRcR8YD60vkBnZtUpc2Rfy8t7NU2hA72NzKwaZYr9HmBfSW+U1A+cRnbXEjPrQW2fxkfEFkkfA/6TrOltQUQ8VLRMX19fYbNBmeaIKVOmtL1s3duue/1FWu2XVrmVbZasyrhx5b5PVufranfbpT6zR8TNtHeHETPrMn9d1iwRLnazRLjYzRLhYjdLhIvdLBEudrNEdPVONXV+XbbKdvg628Gh2tyqbE+ue79VqcrvbbRad0SM2p/dR3azRLjYzRLhYjdLhIvdLBEudrNEuNjNEtFTTW/balfOqrs7nnHGGYXx7bffvmlsxowZhcsedNBBbeU07Pbbm47XCMBPfvKTprGFCxcWLuumudEV7ZeBgQE2b97spjezlLnYzRLhYjdLhIvdLBEudrNEuNjNEuFiN0tET7Wzt9LL7fBlXHHFFYXxE088sbJtV23VqlVNY6eddlrhstKozcV/UvY2169Gbmc3Mxe7WSpc7GaJcLGbJcLFbpYIF7tZIlzsZokoNYrr1mo1ZHOV6hxit2w7eplbC69YsaJw2TvuuKMwvtdeexXGjznmmML43nvv3TR2yimnFC57ww03FMarHC761dhGX6rYJa0CngdeArZExPROJGVmndeJI/vfRMRTHViPmVXIn9nNElG22AP4uaQlkuaMNoOkOZIWS1o8NDRUcnNm1q6yp/FHRMRaSbsBt0j6n4i4s3GGiJgPzAfo7++vbaw3s9SVOrJHxNr85wbgBuDwTiRlZp3XdrFL2lHSzsPPgRnAsk4lZmadVeY0fhJwQ97neDvgexHxHx3JqokqhyYu02bbqp38+OOPbyunYa3uzT579uymsY0bNxYu+8ILLxTG+/r6CuOLFi0qjE+bNq1p7MADDyxctlU7exnbcjt6u98faLvYI2IlUG6EATPrGje9mSXCxW6WCBe7WSJc7GaJcLGbJaKrXVwHBwdfld0ON2/eXGr5Rx55pDA+a9aswviGDRtKbb9Iq+Gi99tvv7bXfeuttxbGy3ZLrnKY7Trfi62GbG7GR3azRLjYzRLhYjdLhIvdLBEudrNEuNjNEuFiN0tEMreSLquobfOWW24pXPbtb397YXzTpk2F8WeffbYwXqWZM2cWxrfbrvgtVGVbdytVrr/scNF13NrcR3azRLjYzRLhYjdLhIvdLBEudrNEuNjNEuFiN0uEIro3SIukyjZWtn9x2XbTKtddZZvsvHnzCuPnnHNOYbyo/zTAfffd1zR26qmnFi774osvFsbr3G9V9mcv834ZGBhg8+bNGi3mI7tZIlzsZolwsZslwsVulggXu1kiXOxmiXCxmyWiq+3s/f39sa32Z99WHX300YXx+fPnF8ZbDdn89NNPF8ZPOumkprFW7cm+b/zoWuUeEe21s0taIGmDpGUN0yZKukXS7/KfE7Y6YzPrqrGcxl8NHDdi2rnAbRGxL3Bb/ruZ9bCWxR4RdwIbR0yeCVyTP78GOLmzaZlZp7V7D7pJEbE+f/4E0PSDuKQ5wByA8ePHt7k5Myur9NX4yK7wNb3KFxHzI2J6REwfN84X/83q0m71DUiaDJD/rG4YUTPriHaL/SZgdv58NnBjZ9Ixs6q0/Mwu6TrgKGBXSWuAzwMXAD+U9EFgNfDeKpO09h100EGF8Vbt6K1cfvnlhfG77767aaxVW3XZ/upF8V68r/tYt93u+Owtiz0iZjUJvbPVsmbWO3zFzCwRLnazRLjYzRLhYjdLhIvdLBFdHbLZqrFgwYKmsSOPPLLUuq+//vrC+IUXXlhq/UXqvL13WXU23TXjI7tZIlzsZolwsZslwsVulggXu1kiXOxmiXCxmyWip9rZy7RN1nlr36rttttuhfHp06c3jfX39xcuu3Tp0sL42WefXRh/4YUXCuNlVD0Md5XbrlK7r8tHdrNEuNjNEuFiN0uEi90sES52s0S42M0S4WI3S0RX29kHBwd7sp9vr5s7d25hfNOmTW3FABYuXFgYX716dWG8jCr7q3di+Sq3XdV3AIpuJe0ju1kiXOxmiXCxmyXCxW6WCBe7WSJc7GaJcLGbJaKn+rNX2S5atk23yuF/Z8yYURg/8MADC+NF2//Rj35UuOzFF19cGK9SL/cZ72WV9WeXtEDSBknLGqbNlbRW0v3544S2tm5mXTOW0/irgeNGmX5JRBycP27ubFpm1mktiz0i7gQ2diEXM6tQmQt0H5O0ND/Nn9BsJklzJC2WtLjEtsyspHaL/RvAPsDBwHrgq81mjIj5ETE9IprfFdHMKtdWsUfEQES8FBFDwDeBwzublpl1WlvFLmlyw6/vBpY1m9fMekPLdnZJ1wFHAbtKWgN8HjhK0sFAAKuAj4xlY319fUyaNKndXGvtC1+mTXjChKaXNAC46KKLCuPbbdf+1yGWLSv+P1zlfd+tt7R8F0XErFEmX1VBLmZWIX9d1iwRLnazRLjYzRLhYjdLhIvdLBE91cW1laLmr7LNcmW6wLZa9txzzy2M77LLLoXxVq688sqmsTq7sKasymZi30razAq52M0S4WI3S4SL3SwRLnazRLjYzRLhYjdLhCKiaxvr7++Poi6uVQ7hW2f32Mcee6wwXqYLK8Bhhx3WNLZhw4ZS626lzttBVzXscdl11y0iNNp0H9nNEuFiN0uEi90sES52s0S42M0S4WI3S4SL3SwRXe3PPjg4uM22X1bZl76sAw44oGls9913L1y2yu82ACxfvrxprFVuTzzxRGF84sSJbeUErb/bcP7557e97rHYsmVL09iXv/zlwmVffPHFtrbpI7tZIlzsZolwsZslwsVulggXu1kiXOxmiXCxmyViLEM2TwWuBSaRDdE8PyK+Lmki8ANgb7Jhm98bEc+USaaX+6vXdR/wsbj99ts7lEnnLVq0qGlsyZIlhcsecsghhfFDDz20rZx63ZNPPlkYv/TSS9ta71iO7FuAT0XENOAvgI9KmgacC9wWEfsCt+W/m1mPalnsEbE+Iu7Nnz8PPAzsAcwErslnuwY4uaIczawDtuozu6S9gUOAXwOTImJ9HnqC7DTfzHrUmL8bL2knYCHwyYh4Tvr/21xFREga9WZ2kuYAc8omambljOnILqmPrNC/GxE/zicPSJqcxycDo97ZMCLmR8T0iJjeiYTNrD0ti13ZIfwq4OGI+FpD6CZgdv58NnBj59Mzs05peStpSUcAvwQeBIbyyeeRfW7/IbAnsJqs6W1j0brK3kq6SC/fGrhoSGWAY489ttT667ydcxlV/01eeumlprGhoaGmMWi9TxcsWFAYf+CBBwrjRdatW1cYv/HG4uNqs1tJt/zMHhF3AaMuDLyz1fJm1hv8DTqzRLjYzRLhYjdLhIvdLBEudrNEuNjNErFNDdn8anXmmWcWxvv6+grjZdrZ999//8L4u971rrbX3crFF19cGH/88cdLrf/mm29uGluxYkWpdZdV1XcjBgYG2Lx5s4dsNkuZi90sES52s0S42M0S4WI3S4SL3SwRLnazRCTTzl62v3svD9lcpSr7yqe834pee9n3arP+7D6ymyXCxW6WCBe7WSJc7GaJcLGbJcLFbpYIF7tZIrrazt5siKhhVQ7ZvK3eW72sXm7L7uV7/depzHvV/dnNzMVulgoXu1kiXOxmiXCxmyXCxW6WCBe7WSJaDtksaSpwLTAJCGB+RHxd0lzgw8CT+aznRUTzG3WT3f+8qD97K2X6AJdZd6v1l20PrvI7AFXvlzLb35a/G9HLf/NmWhY7sAX4VETcK2lnYImkW/LYJRFRfKd/M+sJLYs9ItYD6/Pnz0t6GNij6sTMrLO26jO7pL2BQ4Bf55M+JmmppAWSJjRZZo6kxZIWDw0NlcvWzNo25mKXtBOwEPhkRDwHfAPYBziY7Mj/1dGWi4j5ETE9IqaPG+frgWZ1GVP1SeojK/TvRsSPASJiICJeiogh4JvA4dWlaWZltSx2SQKuAh6OiK81TJ/cMNu7gWWdT8/MOmUsV+P/Cngf8KCk+/Np5wGzJB1M1hy3CvhI2WTqbIqpsrtlL3flrLoJKNVuqq1U2YzczFiuxt8FjNY/trBN3cx6i6+YmSXCxW6WCBe7WSJc7GaJcLGbJcLFbpaIsbSz94w62iY7oez3B7blNv46h7ru5fdLHfvFR3azRLjYzRLhYjdLhIvdLBEudrNEuNjNEuFiN0tEt4dsfhJY3TBpV+CpriWwdXo1t17NC5xbuzqZ214R8frRAl0t9ldsXFocEdNrS6BAr+bWq3mBc2tXt3LzabxZIlzsZomou9jn17z9Ir2aW6/mBc6tXV3JrdbP7GbWPXUf2c2sS1zsZomopdglHSfpt5JWSDq3jhyakbRK0oOS7pe0uOZcFkjaIGlZw7SJkm6R9Lv856hj7NWU21xJa/N9d7+kE2rKbaqkX0haLukhSZ/Ip9e67wry6sp+6/pndknjgUeAY4A1wD3ArIhY3tVEmpC0CpgeEbV/AUPSXwN/BK6NiLfm0y4CNkbEBfk/ygkR8ekeyW0u8Me6h/HORyua3DjMOHAycDo17ruCvN5LF/ZbHUf2w4EVEbEyIjYD3wdm1pBHz4uIO4GNIybPBK7Jn19D9mbpuia59YSIWB8R9+bPnweGhxmvdd8V5NUVdRT7HsDjDb+vobfGew/g55KWSJpTdzKjmBQR6/PnTwCT6kxmFC2H8e6mEcOM98y+a2f487J8ge6VjoiIQ4HjgY/mp6s9KbLPYL3UdjqmYby7ZZRhxv+kzn3X7vDnZdVR7GuBqQ2/T8mn9YSIWJv/3ADcQO8NRT0wPIJu/nNDzfn8SS8N4z3aMOP0wL6rc/jzOor9HmBfSW+U1A+cBtxUQx6vIGnH/MIJknYEZtB7Q1HfBMzOn88Gbqwxl5fplWG8mw0zTs37rvbhzyOi6w/gBLIr8o8Cn60jhyZ5vQl4IH88VHduwHVkp3WDZNc2PgjsAtwG/A64FZjYQ7l9G3gQWEpWWJNryu0IslP0pcD9+eOEuvddQV5d2W/+uqxZInyBziwRLnazRLjYzRLhYjdLhIvdLBEudrNEuNjNEvF/xjIPcX1d+6YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.value.reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"perturbed prediction: {label.value.argmax()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi-processing to get robustness accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verify_utils.verify_utils import verify_batch_images\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare time without multi-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "accuracy:0.966796875\n",
      "time elapsed 962.2611734867096s\n"
     ]
    }
   ],
   "source": [
    "# a seririalized implementation\n",
    "start = time.time()\n",
    "img_number=0\n",
    "verified_number=0\n",
    "imgs,labels=next(iter(test_loader))\n",
    "eps = 0.01\n",
    "model.to(device)\n",
    "img_number += len(imgs)\n",
    "for img in imgs:\n",
    "    img = img.squeeze().to(device)\n",
    "    _,_,indicator = verify_single_image(model=model,image=img,eps=eps)\n",
    "    if indicator==0:\n",
    "        verified_number+=1\n",
    "finish=time.time()\n",
    "print(f\"accuracy:{verified_number/img_number}\")\n",
    "print(f\"time elapsed {finish-start}s\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_num:512,verified_num:495\n"
     ]
    }
   ],
   "source": [
    "print(f\"total_num:{img_number},verified_num:{verified_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Value,Lock\n",
    "import time\n",
    "from verify_utils.verify_utils import verify_batch_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "found solution\n",
      "total_num:512,verified_num:495\n",
      "accuracy:0.966796875\n",
      "time elapsed 547.378613948822s\n"
     ]
    }
   ],
   "source": [
    "# multi processing\n",
    "start = time.time()\n",
    "# step1 get a batch images\n",
    "imgs,labels=next(iter(test_loader))\n",
    "model.to(device)\n",
    "# start from a GPU model and tensor imgs\n",
    "# step2 get the labels for a batch imgs\n",
    "labels=model(imgs.to(device)).argmax(axis=1).cpu().numpy()\n",
    "# step 3 split imgs and labels for each process\n",
    "num_process =10\n",
    "processes=[]\n",
    "num_per_process=int(len(imgs)/num_process)\n",
    "# convert model to cpu\n",
    "model.to(\"cpu\")\n",
    "# define processes\n",
    "lock =Lock()\n",
    "total_num = Value('i',0)\n",
    "verified_num = Value('i',0)\n",
    "eps=0.01\n",
    "for i in range(num_process):\n",
    "    if i!=num_process-1:\n",
    "        process_imgs=imgs[num_per_process*i:num_per_process*(i+1)].squeeze()\n",
    "        process_labels = labels[num_per_process*i:num_per_process*(i+1)]\n",
    "    else:\n",
    "        process_imgs=imgs[num_per_process*i:].squeeze()\n",
    "        process_labels = labels[num_per_process*i:]\n",
    "    p=Process(target=verify_batch_images,args=(model,process_imgs,process_labels,eps,total_num,verified_num,lock))\n",
    "    processes.append(p)\n",
    "    \n",
    "# start\n",
    "for p in processes:\n",
    "    p.start()\n",
    "    \n",
    "# join\n",
    "for p in processes:\n",
    "    p.join()\n",
    "finish = time.time()\n",
    "\n",
    "print(f\"total_num:{total_num.value},verified_num:{verified_num.value}\")\n",
    "print(f\"accuracy:{verified_num.value/total_num.value}\")\n",
    "print(f\"time elapsed {finish-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2  cpu 526.1203780174255s\n",
    "# 4  cpu 534.806479215622s\n",
    "# 10 cpu 548.1252098083496s 547.378613948822s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
