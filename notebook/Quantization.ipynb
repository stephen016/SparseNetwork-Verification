{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.networks.MLP2 import MLP2\n",
    "from models.networks.MLP3 import MLP3\n",
    "from models.networks.ConvertMLP import ConvertMLP2,ConvertMLP3\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start from a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MLP3(input_dim=(28,28),hidden_dim=64,output_dim=10).cpu().eval()\n",
    "# get the model parameters\n",
    "state = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convet the model to a quantizable model\n",
    "c_model= ConvertMLP2(model)\n",
    "# load previous parameter\n",
    "c_model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set quantization config, can also set maunually\n",
    "# see https://github.com/kredde/compression-robustness/blob/e71b2ac493577e8e4b6c7a414dd48850fdeefd63/src/experiments/static_quantization.py#L39\n",
    "c_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuse model\n",
    "## to do compare fuse or not\n",
    "c_model.fuse()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stephen\\Anaconda3\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0389,  0.0415, -0.6710,  0.3382, -0.0209, -0.2361,  0.0457,  0.8677,\n",
       "          0.5616, -1.1181],\n",
       "        [ 0.0525,  0.4014, -0.6845,  0.6979, -0.0962,  0.1083, -0.5209,  0.0190,\n",
       "         -0.1059,  0.7508],\n",
       "        [ 0.9061,  0.1395,  0.0837,  0.3281, -0.0590,  0.5052, -1.0965,  0.5720,\n",
       "          0.5648,  0.3750]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use calibrate date to calibrate model\n",
    "model_prepared = torch.quantization.prepare(c_model)\n",
    "# _input should be some calibration data\n",
    "_input = torch.randn(3,28,28)\n",
    "model_prepared(_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stephen\\Anaconda3\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1\n",
      "C:\\Users\\Stephen\\Anaconda3\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1\n"
     ]
    }
   ],
   "source": [
    "# convert to int8\n",
    "model_int8 = torch.quantization.convert(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvertMLP2(\n",
       "  (layers): Sequential(\n",
       "    (0): QuantizedLinearReLU(in_features=784, out_features=64, scale=0.020231977105140686, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "    (1): Identity()\n",
       "    (2): QuantizedLinear(in_features=64, out_features=64, scale=0.02820894680917263, zero_point=71, qscheme=torch.per_channel_affine)\n",
       "    (3): ReLU()\n",
       "    (4): QuantizedLinear(in_features=64, out_features=10, scale=0.010910957120358944, zero_point=44, qscheme=torch.per_channel_affine)\n",
       "  )\n",
       "  (quant): Quantize(scale=tensor([0.0524]), zero_point=tensor([62]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model_int8(_input)\n",
    "model_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
