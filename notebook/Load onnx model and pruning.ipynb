{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from models.Pruneable import Pruneable\n",
    "import onnx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.model_utils import *\n",
    "from utils.config_utils import *\n",
    "from utils.system_utils import *\n",
    "from models.statistics.Metrics import Metrics\n",
    "from utils.system_utils import setup_directories\n",
    "from verify_utils.onnx_translator import ONNXTranslator\n",
    "from models.networks.MLP5 import MLP5\n",
    "from models import GeneralModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define arguments manually\n",
    "arguments = {}\n",
    "# device\n",
    "arguments['device'] = \"cuda\"\n",
    "\n",
    "# define arguments for model\n",
    "#arguments.model = \"ResNet18\" # ResNet not supported for structured\n",
    "arguments['model'] = \"MLP5\"\n",
    "\n",
    "arguments['disable_masking'] = 1 # 0 for disable mask, 1 for mask (unstructured)\n",
    "arguments['track_weights'] = 0\n",
    "arguments['enable_rewinding'] = 0\n",
    "arguments['growing_rate'] = 0.0000\n",
    "arguments['outer_layer_pruning'] = 0\n",
    "# arguments.prune_criterion = \"SNIPit\"  # unstructured\n",
    "\n",
    "arguments['prune_criterion'] = \"SNAPit\" # or SNAPit ... # structured\n",
    "arguments['l0'] = 0\n",
    "arguments['l0_reg'] = 1.0\n",
    "arguments['l1_reg'] = 0\n",
    "arguments['lp_reg'] = 0\n",
    "arguments['l2_reg'] = 5e-5\n",
    "arguments['hoyer_reg'] = 0.001\n",
    "arguments['N'] = 6000 # different for different dataset\n",
    "arguments['beta_ema'] = 0.999\n",
    "\n",
    "\n",
    "# define arguments for criterion\n",
    "arguments['pruning_limit'] = 0.9\n",
    "arguments['snip_steps'] = 6\n",
    "\n",
    "# not pre-trained model\n",
    "arguments['checkpoint_name'] = None\n",
    "arguments['checkpoint_model'] = None\n",
    "\n",
    "# dataset\n",
    "arguments['data_set'] = \"MNIST\"\n",
    "arguments['batch_size'] = 512\n",
    "arguments['mean'] = (0.1307,)\n",
    "arguments['std'] = (0.3081,)\n",
    "arguments['tuning'] = 0\n",
    "arguments['preload_all_data'] = 0\n",
    "arguments['random_shuffle_labels'] = 0\n",
    "\n",
    "# loss\n",
    "arguments['loss'] = \"CrossEntropy\"\n",
    "\n",
    "# optimizer\n",
    "arguments['optimizer'] = \"ADAM\"\n",
    "arguments['learning_rate'] = 2e-3\n",
    "\n",
    "# training\n",
    "arguments['save_freq'] = 10\n",
    "arguments['eval'] = 0\n",
    "arguments['train_scheme'] = \"DefaultTrainer\"\n",
    "arguments['seed'] = 1234\n",
    "arguments['epochs'] = 20\n",
    "\n",
    "arguments['grad_noise'] = 0\n",
    "arguments['grad_clip'] =10\n",
    "arguments['eval_freq'] = 1000\n",
    "arguments['max_training_minutes']= 6120\n",
    "arguments['plot_weights_freq'] = 50\n",
    "arguments['prune_delay'] = 0\n",
    "arguments['prune_freq'] = 1\n",
    "arguments['rewind_to'] = 6\n",
    "\n",
    "arguments['skip_first_plot'] = 0\n",
    "arguments['disable_histograms'] = 0\n",
    "arguments['disable_saliency'] = 0\n",
    "arguments['disable_confusion'] = 0\n",
    "arguments['disable_weightplot'] = 0\n",
    "arguments['disable_netplot'] = 0\n",
    "arguments['disable_activations'] = 0\n",
    "\n",
    "arguments['pruning_rate'] = 0\n",
    "# during training\n",
    "arguments['pruning_freq'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at 2022-06-01_18.25.45\n"
     ]
    }
   ],
   "source": [
    "metrics = Metrics()\n",
    "out = metrics.log_line\n",
    "print = out\n",
    "\n",
    "ensure_current_directory()\n",
    "global out \n",
    "out = metrics.log_line\n",
    "out(f\"starting at {get_date_stamp()}\")\n",
    "\n",
    "metrics._batch_size = arguments['batch_size']\n",
    "metrics._eval_freq = arguments['eval_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = configure_device(arguments)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/homedirs/wangxun/robustness/SparseNetwork-Verification\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model=onnx.load(\"notebook/mnist_relu_5_100.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = ONNXTranslator(onnx_model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of [0.] extracted from network\n",
      "Std of [1.] extracted from network\n"
     ]
    }
   ],
   "source": [
    "operations, resources = translator.translate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: GeneralModel = find_right_model(\n",
    "        NETWORKS_DIR,arguments['model'],\n",
    "        device=device,\n",
    "        operation=operations,\n",
    "        resources=resources,\n",
    "        is_maskable=arguments['disable_masking'],\n",
    "        is_tracking_weights=arguments['track_weights'],\n",
    "        is_rewindable=arguments['enable_rewinding'],\n",
    "        is_growable=arguments['growing_rate'] > 0,\n",
    "        outer_layer_pruning=arguments['outer_layer_pruning'],\n",
    "        maintain_outer_mask_anyway=(\n",
    "                                       not arguments['outer_layer_pruning']) and (\n",
    "                                           \"Structured\" in arguments['prune_criterion']),\n",
    "        l0=arguments['l0'],\n",
    "        l0_reg=arguments['l0_reg'],\n",
    "        N=arguments['N'],\n",
    "        beta_ema=arguments['beta_ema'],\n",
    "        l2_reg=arguments['l2_reg']\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP5(\n",
       "  (layers): Sequential(\n",
       "    (0): ContainerLinear(in_features=784, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): ContainerLinear(in_features=100, out_features=100, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): ContainerLinear(in_features=100, out_features=100, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): ContainerLinear(in_features=100, out_features=100, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): ContainerLinear(in_features=100, out_features=100, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): ContainerLinear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prune the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mean (0.1307,)\n",
      "Made datestamp: 2022-06-01_18.25.55_model=MLP5_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.9_train-scheme=DefaultTrainer_seed=1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/homedirs/wangxun/miniconda3/envs/gr/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# get criterion\n",
    "criterion = find_right_model(\n",
    "        CRITERION_DIR,arguments['prune_criterion'],\n",
    "        model=model,\n",
    "        limit=arguments['pruning_limit'],\n",
    "        start=0.5,\n",
    "        steps=arguments['snip_steps'],\n",
    "        device=arguments['device']\n",
    "    )   \n",
    "# load data\n",
    "train_loader, test_loader = find_right_model(\n",
    "        DATASETS, arguments['data_set'],\n",
    "        arguments=arguments,\n",
    "        mean=arguments['mean'],\n",
    "        std=arguments['std']\n",
    "    )\n",
    "# get loss function\n",
    "loss = find_right_model(\n",
    "        LOSS_DIR, arguments['loss'],\n",
    "        device=device,\n",
    "        l1_reg=arguments['l1_reg'],\n",
    "        lp_reg=arguments['lp_reg'],\n",
    "        l0_reg=arguments['l0_reg'],\n",
    "        hoyer_reg=arguments['hoyer_reg']\n",
    "    )\n",
    "# get optimizer\n",
    "optimizer = find_right_model(\n",
    "        OPTIMS, arguments['optimizer'],\n",
    "        params=model.parameters(),\n",
    "        lr=arguments['learning_rate'],\n",
    "        weight_decay=arguments['l2_reg'] if not arguments['l0'] else 0\n",
    "    )\n",
    "if not arguments['eval']:\n",
    "    # build trainer\n",
    "    run_name = f\"_model={arguments['model']}_dataset={arguments['data_set']}_prune-criterion={arguments['prune_criterion']}\" + \\\n",
    "               f\"_pruning-limit={arguments['pruning_limit']}_train-scheme={arguments['train_scheme']}_seed={arguments['seed']}\"\n",
    "    trainer = find_right_model(\n",
    "            TRAINERS_DIR, arguments['train_scheme'],\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            arguments=arguments,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            metrics=metrics,\n",
    "            criterion=criterion,\n",
    "            run_name = run_name\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model before training\n",
    "setup_directories()\n",
    "save_models([model],\"original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mStarted training\u001b[0m\n",
      "Saved results/2022-06-01_18.25.55_model=MLP5_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.9_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 34496 percentage 0.44 length_nonzero 78400\n",
      "pruning 7480 percentage 0.748 length_nonzero 10000\n",
      "pruning 7930 percentage 0.793 length_nonzero 10000\n",
      "pruning 7470 percentage 0.747 length_nonzero 10000\n",
      "pruning 7360 percentage 0.736 length_nonzero 10000\n",
      "pruning 520 percentage 0.52 length_nonzero 1000\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=56.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=56.0, out_features=45.0, bias=True)\n",
      "    (4): ContainerLinear(in_features=45.0, out_features=46.0, bias=True)\n",
      "    (6): ContainerLinear(in_features=46.0, out_features=55.0, bias=True)\n",
      "    (8): ContainerLinear(in_features=55.0, out_features=48.0, bias=True)\n",
      "    (10): ContainerLinear(in_features=48.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.5465326633165829\n",
      "Saved results/2022-06-01_18.25.55_model=MLP5_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.9_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 15680 percentage 0.35714285714285715 length_nonzero 43904\n",
      "pruning 1440 percentage 0.5714285714285714 length_nonzero 2520\n",
      "pruning 1170 percentage 0.5652173913043478 length_nonzero 2070\n",
      "pruning 1690 percentage 0.6679841897233202 length_nonzero 2530\n",
      "pruning 1912 percentage 0.7242424242424242 length_nonzero 2640\n",
      "pruning 220 percentage 0.4583333333333333 length_nonzero 480\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=36.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=36.0, out_features=30.0, bias=True)\n",
      "    (4): ContainerLinear(in_features=30.0, out_features=30.0, bias=True)\n",
      "    (6): ContainerLinear(in_features=30.0, out_features=28.0, bias=True)\n",
      "    (8): ContainerLinear(in_features=28.0, out_features=26.0, bias=True)\n",
      "    (10): ContainerLinear(in_features=26.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.4083924349881797\n",
      "Saved results/2022-06-01_18.25.55_model=MLP5_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.9_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 10192 percentage 0.3611111111111111 length_nonzero 28224\n",
      "pruning 620 percentage 0.5740740740740741 length_nonzero 1080\n",
      "pruning 560 percentage 0.6222222222222222 length_nonzero 900\n",
      "pruning 483 percentage 0.575 length_nonzero 840\n",
      "pruning 329 percentage 0.4519230769230769 length_nonzero 728\n",
      "pruning 70 percentage 0.2692307692307692 length_nonzero 260\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=23.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=23.0, out_features=20.0, bias=True)\n",
      "    (4): ContainerLinear(in_features=20.0, out_features=17.0, bias=True)\n",
      "    (6): ContainerLinear(in_features=17.0, out_features=21.0, bias=True)\n",
      "    (8): ContainerLinear(in_features=21.0, out_features=19.0, bias=True)\n",
      "    (10): ContainerLinear(in_features=19.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.38255494505494503\n",
      "Saved results/2022-06-01_18.25.55_model=MLP5_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.9_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 4704 percentage 0.2608695652173913 length_nonzero 18032\n",
      "pruning 239 percentage 0.5195652173913043 length_nonzero 460\n",
      "pruning 158 percentage 0.4647058823529412 length_nonzero 340\n",
      "pruning 147 percentage 0.4117647058823529 length_nonzero 357\n",
      "pruning 159 percentage 0.39849624060150374 length_nonzero 399\n",
      "pruning 30 percentage 0.15789473684210525 length_nonzero 190\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=17.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=17.0, out_features=13.0, bias=True)\n",
      "    (4): ContainerLinear(in_features=13.0, out_features=14.0, bias=True)\n",
      "    (6): ContainerLinear(in_features=14.0, out_features=15.0, bias=True)\n",
      "    (8): ContainerLinear(in_features=15.0, out_features=16.0, bias=True)\n",
      "    (10): ContainerLinear(in_features=16.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.27490140560218423\n",
      "Saved results/2022-06-01_18.25.55_model=MLP5_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.9_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 1568 percentage 0.11764705882352941 length_nonzero 13328\n",
      "pruning 56 percentage 0.25339366515837103 length_nonzero 221\n",
      "pruning 72 percentage 0.3956043956043956 length_nonzero 182\n",
      "pruning 90 percentage 0.42857142857142855 length_nonzero 210\n",
      "pruning 72 percentage 0.3 length_nonzero 240\n",
      "pruning 20 percentage 0.125 length_nonzero 160\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=15.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=15.0, out_features=11.0, bias=True)\n",
      "    (4): ContainerLinear(in_features=11.0, out_features=10.0, bias=True)\n",
      "    (6): ContainerLinear(in_features=10.0, out_features=12.0, bias=True)\n",
      "    (8): ContainerLinear(in_features=12.0, out_features=14.0, bias=True)\n",
      "    (10): ContainerLinear(in_features=14.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.13095321107314692\n",
      "Saved results/2022-06-01_18.25.55_model=MLP5_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.9_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 1568 percentage 0.13333333333333333 length_nonzero 11760\n",
      "pruning 22 percentage 0.13333333333333333 length_nonzero 165\n",
      "pruning 11 percentage 0.1 length_nonzero 110\n",
      "pruning 39 percentage 0.325 length_nonzero 120\n",
      "pruning 42 percentage 0.25 length_nonzero 168\n",
      "pruning 0 percentage 0.0 length_nonzero 140\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=13.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=13.0, out_features=11.0, bias=True)\n",
      "    (4): ContainerLinear(in_features=11.0, out_features=9.0, bias=True)\n",
      "    (6): ContainerLinear(in_features=9.0, out_features=9.0, bias=True)\n",
      "    (8): ContainerLinear(in_features=9.0, out_features=14.0, bias=True)\n",
      "    (10): ContainerLinear(in_features=14.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.1349594800609805\n",
      "Saved results/2022-06-01_18.25.55_model=MLP5_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.9_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 0 percentage 0.0 length_nonzero 10192\n",
      "pruning 39 percentage 0.2727272727272727 length_nonzero 143\n",
      "pruning 27 percentage 0.2727272727272727 length_nonzero 99\n",
      "pruning 0 percentage 0.0 length_nonzero 81\n",
      "pruning 0 percentage 0.0 length_nonzero 126\n",
      "pruning 0 percentage 0.0 length_nonzero 140\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=13.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=13.0, out_features=8.0, bias=True)\n",
      "    (4): ContainerLinear(in_features=8.0, out_features=9.0, bias=True)\n",
      "    (6): ContainerLinear(in_features=9.0, out_features=9.0, bias=True)\n",
      "    (8): ContainerLinear(in_features=9.0, out_features=14.0, bias=True)\n",
      "    (10): ContainerLinear(in_features=14.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.006121881087097672\n",
      "Saved results/2022-06-01_18.25.55_model=MLP5_dataset=MNIST_prune-criterion=SNAPit_pruning-limit=0.9_train-scheme=DefaultTrainer_seed=1234/output/scores\n",
      "pruning 784 percentage 0.07692307692307693 length_nonzero 10192\n",
      "pruning 8 percentage 0.07692307692307693 length_nonzero 104\n",
      "pruning 8 percentage 0.1111111111111111 length_nonzero 72\n",
      "pruning 9 percentage 0.1111111111111111 length_nonzero 81\n",
      "pruning 18 percentage 0.14285714285714285 length_nonzero 126\n",
      "pruning 20 percentage 0.14285714285714285 length_nonzero 140\n",
      "  (layers): Sequential(\n",
      "    (0): ContainerLinear(in_features=784, out_features=12.0, bias=True)\n",
      "    (2): ContainerLinear(in_features=12.0, out_features=8.0, bias=True)\n",
      "    (4): ContainerLinear(in_features=8.0, out_features=8.0, bias=True)\n",
      "    (6): ContainerLinear(in_features=8.0, out_features=9.0, bias=True)\n",
      "    (8): ContainerLinear(in_features=9.0, out_features=12.0, bias=True)\n",
      "    (10): ContainerLinear(in_features=12.0, out_features=10, bias=True)\n",
      "final percentage after snap: 0.07904806346243584\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 0 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.0957031  |  2.3422835   |  2.2636239  | 0.1032169  |    0.9173534    |   0.9020000   |  0.1855558  |       12.7237766       |    1.1472305    |      10319.0000000      |      6.7229076      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0423804     |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... 117/118\n",
      "\n",
      "plotting..\n",
      "finished plotting\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 1 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.2089844  |  1.8764400   |  1.8421321  | 0.2212718  |    0.9173534    |   0.9020000   |  0.3565431  |       12.7237766       |    6.0786732    |      10319.0000000      |      8.7984545      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0131503     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 2 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.3105469  |  1.6389621   |  1.6840366  | 0.2923713  |    0.9173534    |   0.9020000   |  0.4434196  |       12.7237766       |    5.6877959    |      10319.0000000      |      9.0976559      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0169427     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 3 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.4375000  |  1.4341744   |  1.4478989  | 0.4486558  |    0.9173534    |   0.9020000   |  0.6025961  |       12.7237766       |    6.7467543    |      10319.0000000      |      9.2731359      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0134280     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 4 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.6425781  |  1.1028284   |  1.1007320  | 0.6409524  |    0.9173534    |   0.9020000   |  0.7546399  |       12.7237766       |    5.6452580    |      10319.0000000      |      9.3977687      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0162756     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 5 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.7441406  |  0.8428523   |  0.8602120  | 0.7364143  |    0.9173534    |   0.9020000   |  0.8169856  |       12.7237766       |    5.6210061    |      10319.0000000      |      9.4944951      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0157982     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 6 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.8164062  |  0.6367729   |  0.6896813  | 0.7944049  |    0.9173534    |   0.9020000   |  0.8514637  |       12.7237766       |    6.1486618    |      10319.0000000      |      9.5735538      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0122627     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 7 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.8945312  |  0.4865986   |  0.5461134  | 0.8828642  |    0.9173534    |   0.9020000   |  0.8997784  |       12.7237766       |    4.9757584    |      10319.0000000      |      9.6404131      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0148666     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 8 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9003906  |  0.3612130   |  0.4376994  | 0.8904125  |    0.9173534    |   0.9020000   |  0.9036822  |       12.7237766       |    4.9603093    |      10319.0000000      |      9.6983394      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0178162     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 9 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9003906  |  0.3046528   |  0.3980887  | 0.9016487  |    0.9173534    |   0.9020000   |  0.9094333  |       12.7237766       |    6.7719260    |      10319.0000000      |      9.7494408      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0149347     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 10 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9238281  |  0.2919711   |  0.3991147  | 0.9005630  |    0.9173534    |   0.9020000   |  0.9088807  |       12.7237766       |    5.1790722    |      10319.0000000      |      9.7951575      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0215196     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "SAVING...\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 11 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9101562  |  0.3403924   |  0.3835524  | 0.9106560  |    0.9173534    |   0.9020000   |  0.9139925  |       12.7237766       |    6.8174198    |      10319.0000000      |      9.8365167      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0195150     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 12 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9316406  |  0.2488619   |  0.3428111  | 0.9195830  |    0.9173534    |   0.9020000   |  0.9184668  |       12.7237766       |    7.0538945    |      10319.0000000      |      9.8742774      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0131649     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 13 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9394531  |  0.2086767   |  0.3347428  | 0.9182847  |    0.9173534    |   0.9020000   |  0.9178188  |       12.7237766       |    5.8487961    |      10319.0000000      |      9.9090160      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0147089     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 14 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9492188  |  0.1774054   |  0.3363925  | 0.9170669  |    0.9173534    |   0.9020000   |  0.9172101  |       12.7237766       |    5.1023506    |      10319.0000000      |      9.9411804      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0122697     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 15 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9296875  |  0.2881905   |  0.3284730  | 0.9157341  |    0.9173534    |   0.9020000   |  0.9165431  |       12.7237766       |    6.5952584    |      10319.0000000      |      9.9711261      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0127646     |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 16 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9511719  |  0.2062275   |  0.3299393  | 0.9189338  |    0.9173534    |   0.9020000   |  0.9181429  |       12.7237766       |    5.0909574    |      10319.0000000      |      9.9991395      \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0171713     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 17 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9101562  |  0.2992357   |  0.3152551  | 0.9195944  |    0.9173534    |   0.9020000   |  0.9184726  |       12.7237766       |    6.0467673    |      10319.0000000      |      10.0254549     \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0123987     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 18 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9550781  |  0.1333931   |  0.3187713  | 0.9200827  |    0.9173534    |   0.9020000   |  0.9187160  |       12.7237766       |    5.9811532    |      10319.0000000      |      10.0502665     \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0169303     |\n",
      "Training... 117/118\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mEPOCH 19 \u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training... 0/118\n",
      "\n",
      "Evaluating... 19/20\n",
      "\n",
      "$  acc/train  |  loss/train  |  loss/test  |  acc/test  |  sparse/weight  |  sparse/node  |  sparse/hm  |  sparse/log_disk_size  |  time/gpu_time  |  time/flops_per_sample  |  time/flops_log_cum \n",
      "$  0.9375000  |  0.2766476   |  0.3051797  | 0.9243164  |    0.9173534    |   0.9020000   |  0.9208218  |       12.7237766       |    6.6798961    |      10319.0000000      |      10.0737368     \n",
      "$ |  cuda/ram_footprint  |  time/batch_time  |  \n",
      "$ |    227328.0000000    |     0.0114641     |\n",
      "Training... 117/118\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
